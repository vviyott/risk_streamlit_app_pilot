# utils/fda_realtime_crawler.py
"""
ì‹¤ì‹œê°„ FDA ë¦¬ì½œ ë°ì´í„° í¬ë¡¤ë§ ë° ì—…ë°ì´íŠ¸ ëª¨ë“ˆ - Selenium ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •
"""
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import time
import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd

def create_recall_chunks(text, chunk_size=800, overlap_size=120):
    """ë¦¬ì½œ Company Announcement í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)"""
    
    if not text or len(text.strip()) < 100:
        return []
    
    def protect_important_info(text):
        """ì¤‘ìš”í•œ ì •ë³´ ë³´í˜¸ (íšŒì‚¬ëª…, ì œí’ˆëª…, ë‚ ì§œ ë“±)"""
        patterns = [
            r'[A-Z][a-zA-Z\s&.,]+ LLC',  # íšŒì‚¬ëª…
            r'[A-Z][a-zA-Z\s&.,]+ Inc\.',  # íšŒì‚¬ëª…
            r'[A-Z][a-zA-Z\s&.,]+ Company',  # íšŒì‚¬ëª…
            r'\d+\s*boxes?\s*of\s*[^,]+',  # ì œí’ˆ ìˆ˜ëŸ‰
            r'Lot\s*#?\s*\d+',  # ë¡œíŠ¸ ë²ˆí˜¸
            r'1-\d{3}-\d{3}-\d{4}',  # ì „í™”ë²ˆí˜¸
        ]
        
        protected_refs = {}
        protected_text = text
        
        for i, pattern in enumerate(patterns):
            matches = list(re.finditer(pattern, protected_text, re.IGNORECASE))
            for j, match in enumerate(matches):
                ref_id = f"__PROTECT_{i}_{j}__"
                protected_refs[ref_id] = match.group()
                protected_text = protected_text.replace(match.group(), ref_id, 1)
        
        return protected_text, protected_refs
    
    def restore_protected_info(text, protected_refs):
        """ë³´í˜¸ëœ ì •ë³´ ë³µì›"""
        for ref_id, original_text in protected_refs.items():
            text = text.replace(ref_id, original_text)
        return text
    
    # ì •ë³´ ë³´í˜¸
    protected_text, protected_refs = protect_important_info(text)
    
    # ì²­í¬ ìƒì„±
    chunks = []
    start = 0
    min_chunk_size = 150
    max_chunk_size = 1200
    
    while start < len(protected_text):
        end = start + chunk_size
        
        if end >= len(protected_text):
            chunk = protected_text[start:]
            if chunk.strip() and len(chunk.strip()) >= min_chunk_size:
                restored_chunk = restore_protected_info(chunk.strip(), protected_refs)
                chunks.append(restored_chunk)
            break
        
        # ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
        chunk_text = protected_text[start:end]
        split_candidates = []
        
        separators = ['. ', '.\n', ';\n', '; ', ',\n', ', ', ')\n', ') ', '\n\n', '\n', ' ']
        
        for sep in separators:
            last_sep_pos = chunk_text.rfind(sep)
            if last_sep_pos > chunk_size * 0.7:
                split_candidates.append(last_sep_pos + len(sep))
        
        if split_candidates:
            actual_end = start + max(split_candidates)
        else:
            actual_end = min(end, start + max_chunk_size)
        
        chunk = protected_text[start:actual_end].strip()
        if chunk and len(chunk) >= min_chunk_size:
            restored_chunk = restore_protected_info(chunk, protected_refs)
            chunks.append(restored_chunk)
        
        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
        start = max(actual_end - overlap_size, start + min_chunk_size)
    
    return chunks

# utils/fda_realtime_crawler.py ì˜ FDARealtimeCrawler í´ë˜ìŠ¤ ìˆ˜ì •

class FDARealtimeCrawler:
    def __init__(self):
        self.base_url = "https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts"
        self.driver = None
        
    def _init_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” - ì—ëŸ¬ ë©”ì‹œì§€ ìˆ¨ê¹€"""
        if self.driver is None:
            service = Service(ChromeDriverManager().install())
            options = webdriver.ChromeOptions()
            options.add_argument('--headless')
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-logging')
            options.add_argument('--log-level=3')
            options.add_argument('--silent')
            options.add_argument('--disable-web-security')
            options.add_experimental_option('excludeSwitches', ['enable-logging'])
            options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(service=service, options=options)

    def _close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ - ëˆ„ë½ëœ ë©”ì„œë“œ ì¶”ê°€"""
        if self.driver:
            try:
                self.driver.quit()
            except Exception as e:
                print(f"ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.driver = None

    def check_food_beverages_in_summary(self, url):
        try:
            self.driver.get(url)
            time.sleep(1.5)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # Summary ì„¹ì…˜ì—ì„œ Product Typeë§Œ í™•ì¸
            summary_section = soup.find('h2', string='Summary')
            if not summary_section:
                print(f"      âŒ Summary ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            summary_content = summary_section.find_next('div', class_='inset-column')
            if not summary_content:
                print(f"      âŒ Summary ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # Product Type ì°¾ê¸°
            for dt in summary_content.find_all('dt'):
                if 'Product Type' in dt.get_text():
                    dd = dt.find_next('dd')
                    if dd:
                        product_type = dd.get_text().strip()
                        print(f"      ğŸ” Product Type: '{product_type}'")
                        
                        # ì •í™•íˆ "Food & Beverages" í¬í•¨ ì—¬ë¶€ë§Œ í™•ì¸
                        if 'Food & Beverages' in product_type:
                            print(f"      âœ… Food & Beverages í™•ì¸ë¨")
                            return True
                        else:
                            print(f"      âŒ Food & Beverages ì•„ë‹˜")
                            return False
            
            print(f"      âŒ Product Type í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
            
        except Exception as e:
            print(f"      âŒ Product Type í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_existing_urls_from_vectorstore(self, vectorstore):
        try:
            existing_data = vectorstore.get()
            existing_urls = set()
            for metadata in existing_data.get('metadatas', []):
                if metadata and 'url' in metadata:
                    existing_urls.add(metadata['url'])
            print(f"ğŸ“‹ ê¸°ì¡´ ë²¡í„°DB URL: {len(existing_urls)}ê°œ")
            return existing_urls
        except Exception as e:
            print(f"ê¸°ì¡´ URL í™•ì¸ ì˜¤ë¥˜: {e}")
            return set()
    
    def crawl_latest_recalls(self, days_back: int = 15, vectorstore=None) -> List[Dict]:
        """ìµœì í™”ëœ ë¦¬ì½œ ë°ì´í„° í¬ë¡¤ë§ - ë‚ ì§œ í•„í„° ìˆ˜ì •"""
        recalls = []
        
        try:
            self._init_driver()
            
            # ê¸°ì¡´ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬ìš©)
            existing_urls = set()
            if vectorstore:
                existing_urls = self.get_existing_urls_from_vectorstore(vectorstore)
            
            self.driver.get(self.base_url)
            time.sleep(2)
            
            print(f"ìµœê·¼ {days_back}ì¼ê°„ì˜ ìƒˆë¡œìš´ Food & Beverages ë¦¬ì½œ ìˆ˜ì§‘ ì¤‘...")
            print(f"ì œì™¸í•  ê¸°ì¡´ URL: {len(existing_urls)}ê°œ")
            
            # ë‚ ì§œ í•„í„°ë§ ìˆ˜ì • - ë” ê´€ëŒ€í•˜ê²Œ
            cutoff_date = datetime.now() - timedelta(days=days_back + 1)  # í•˜ë£¨ ë” ì—¬ìœ 
            print(f"ìˆ˜ì§‘ ê¸°ì¤€ì¼: {cutoff_date.strftime('%Y-%m-%d')} ì´í›„")
            
            processed_urls = set()
            max_pages = 2
            
            for page in range(1, max_pages + 1):
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                
                # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸°
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.ID, "datatable"))
                    )
                    time.sleep(0.5)
                except TimeoutException:
                    print(f"í˜ì´ì§€ {page} í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨")
                    break
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ì½œ ë§í¬ ìˆ˜ì§‘ ë° ì¤‘ë³µ í•„í„°ë§
                new_recall_links = []
                try:
                    table = self.driver.find_element(By.ID, "datatable")
                    rows = table.find_elements(By.XPATH, ".//tbody/tr")
                    
                    for row in rows:
                        try:
                            # Brand Name ë§í¬ ì¶”ì¶œ (2ë²ˆì§¸ td)
                            brand_cell = row.find_elements(By.TAG_NAME, "td")[1]
                            link_element = brand_cell.find_element(By.TAG_NAME, "a")
                            recall_url = link_element.get_attribute('href')
                            
                            # ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´ DB + í˜„ì¬ ì²˜ë¦¬ëœ URL)
                            if (recall_url and 
                                recall_url not in existing_urls and 
                                recall_url not in processed_urls):
                                new_recall_links.append(recall_url)
                                processed_urls.add(recall_url)
                                
                        except Exception:
                            continue
                    
                    print(f"í˜ì´ì§€ {page}: ìƒˆë¡œìš´ URL {len(new_recall_links)}ê°œ ë°œê²¬")
                    
                except Exception as e:
                    print(f"í˜ì´ì§€ {page} ë§í¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    break
                
                # ìƒˆë¡œìš´ URLì´ ì—†ìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ
                if not new_recall_links:
                    print("ìƒˆë¡œìš´ URLì´ ì—†ì–´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
                    continue
                
                # ê°œë³„ ë¦¬ì½œ í˜ì´ì§€ ì²˜ë¦¬
                food_found_count = 0
                for i, recall_url in enumerate(new_recall_links[:8]):
                    try:
                        print(f"  ê²€ì‚¬ ì¤‘ ({i+1}/{min(8, len(new_recall_links))}): {recall_url[-30:]}...")
                        
                        # Food & Beverages ì—¬ë¶€ ë¨¼ì € í™•ì¸
                        if not self.check_food_beverages_in_summary(recall_url):
                            print(f"    âŒ Food & Beverages ì•„ë‹˜")
                            continue
                        
                        # Food & Beveragesì¸ ê²½ìš° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                        recall_data = self.extract_recall_metadata(recall_url)
                        
                        if recall_data:
                            # ë‚ ì§œ í•„í„°ë§ - ë” ê´€ëŒ€í•˜ê²Œ ì ìš©
                            should_skip = False
                            if recall_data['effective_date']:
                                try:
                                    recall_date = datetime.strptime(recall_data['effective_date'], '%Y-%m-%d')
                                    if recall_date < cutoff_date:
                                        print(f"    â© ë‚ ì§œ í•„í„°ë§: {recall_data['effective_date']} (ê¸°ì¤€: {cutoff_date.strftime('%Y-%m-%d')})")
                                        should_skip = True
                                except Exception as e:
                                    print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            
                            if not should_skip:
                                print(f"    âœ… Food & Beverages ìˆ˜ì§‘: {recall_data['title'][:40]}...")
                                recalls.append(recall_data)
                                food_found_count += 1
                                
                                # ì¶©ë¶„íˆ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
                                if len(recalls) >= 5:
                                    print(f"ëª©í‘œ ë‹¬ì„±: {len(recalls)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                                    return recalls
                        
                    except Exception as e:
                        print(f"    âŒ ë¦¬ì½œ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"í˜ì´ì§€ {page} ì™„ë£Œ: Food & Beverages {food_found_count}ê±´ ë°œê²¬")
                
                # ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                if food_found_count > 0 and len(recalls) >= 3:
                    print("ì¶©ë¶„í•œ ìƒˆ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì¤‘ë‹¨")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if page < max_pages:
                    try:
                        self.driver.get(self.base_url)
                        time.sleep(1.5)
                        
                        WebDriverWait(self.driver, 8).until(
                            EC.presence_of_element_located((By.ID, "datatable"))
                        )
                        time.sleep(0.5)
                        
                        # Next ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ í˜ì´ì§€ ì´ë™
                        for _ in range(page):
                            next_button = WebDriverWait(self.driver, 8).until(
                                EC.element_to_be_clickable((By.ID, "datatable_next"))
                            )
                            
                            if "disabled" in (next_button.get_attribute("class") or ""):
                                print("ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                                return recalls
                            
                            next_link = next_button.find_element(By.TAG_NAME, "a")
                            self.driver.execute_script("arguments[0].click();", next_link)
                            time.sleep(0.8)
                            
                    except Exception as e:
                        print(f"í˜ì´ì§€ ì´ë™ ì˜¤ë¥˜: {e}")
                        break
            
            print(f"í¬ë¡¤ë§ ì™„ë£Œ: ìƒˆë¡œìš´ Food & Beverages ë¦¬ì½œ {len(recalls)}ê±´ ìˆ˜ì§‘")
            return recalls
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
            return []
            
        finally:
            self._close_driver()

    def extract_recall_metadata(self, url):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - ì´ë¯¸ Food & Beverages í™•ì¸ë¨"""
        # ì´ë¯¸ check_food_beverages_in_summaryì—ì„œ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì¶”ì¶œ
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        
        # 1. ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = [
            'h1.content-title', 'h1[class*="content-title"]', 'h1'
        ]
        
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                title = title_element.get_text().strip()
                break
        
        # 2. ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        effective_date = ""
        last_updated = ""
        
        def parse_date_text(date_text):
            if not date_text:
                return ""
            try:
                date_obj = datetime.strptime(date_text.strip(), '%B %d, %Y')
                return date_obj.strftime('%Y-%m-%d')
            except:
                return ""
        
        # Summary ì„¹ì…˜ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        summary_section = soup.find('h2', string='Summary')
        if summary_section:
            summary_content = summary_section.find_next('div', class_='inset-column')
            if summary_content:
                for dt in summary_content.find_all('dt'):
                    if 'Company Announcement Date' in dt.get_text():
                        dd = dt.find_next('dd')
                        if dd:
                            time_element = dd.find('time')
                            if time_element:
                                effective_date = parse_date_text(time_element.get_text())
                            break
                
                for dt in summary_content.find_all('dt'):
                    if 'FDA Publish Date' in dt.get_text():
                        dd = dt.find_next('dd')
                        if dd:
                            time_element = dd.find('time')
                            if time_element:
                                last_updated = parse_date_text(time_element.get_text())
                            break
        
        # 3. Company Announcement ì¶”ì¶œ
        company_announcement = ""
        announcement_section = soup.find('h2', string='Company Announcement')
        if announcement_section:
            current = announcement_section.find_next_sibling()
            announcement_parts = []
            
            while current and current.name != 'hr':
                if current.name == 'p':
                    text = current.get_text().strip()
                    if text:
                        announcement_parts.append(text)
                current = current.find_next_sibling()
            
            company_announcement = '\n\n'.join(announcement_parts)
        
        # 4. ì²­í¬ ìƒì„±
        content_chunks = create_recall_chunks(company_announcement) if company_announcement else []
        
        return {
            "document_type": "recall",
            "category": "Food & Beverages",
            "title": title,
            "url": url,
            "effective_date": effective_date,
            "last_updated": last_updated,
            "chunks": content_chunks
        }

def update_vectorstore_with_new_data(new_recalls: List[Dict], vectorstore) -> int:
    """ìƒˆë¡œìš´ ë¦¬ì½œ ë°ì´í„°ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ - ê°œì„ ëœ ë²„ì „"""
    if not new_recalls or not vectorstore:
        print("âš ï¸ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤")
        return 0
    
    try:
        # 1. ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ URL í™•ì¸
        existing_urls = set()
        try:
            existing_data = vectorstore.get()
            for metadata in existing_data.get('metadatas', []):
                if metadata and 'url' in metadata:
                    existing_urls.add(metadata['url'])
            print(f"ğŸ“‹ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´: {len(existing_urls)}ê°œ URL í™•ì¸")
        except Exception as e:
            print(f"ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # 2. ìƒˆ ë¬¸ì„œë“¤ ìƒì„±
        new_documents = []
        processed_urls = set()
        
        for recall in new_recalls:
            recall_url = recall.get('url', '')
            
            # ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´ ë°ì´í„° ë° í˜„ì¬ ë°°ì¹˜ ë‚´)
            if recall_url in existing_urls or recall_url in processed_urls:
                print(f"â© ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {recall.get('title', '')[:50]}...")
                continue
            
            processed_urls.add(recall_url)
            
            # chunksë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬
            chunks = recall.get("chunks", [])
            if not chunks:
                print(f"âš ï¸ ì²­í¬ ì—†ìŒ: {recall.get('title', '')}")
                continue
                
            for i, chunk_content in enumerate(chunks):
                # ë¹ˆ ë‚´ìš© ê±´ë„ˆë›°ê¸°
                if not chunk_content or len(chunk_content.strip()) < 30:
                    continue
                
                # êµ¬ì¡°í™”ëœ ì»¨í…ì¸  ìƒì„± (ê¸°ì¡´ í˜•ì‹ê³¼ ë™ì¼)
                structured_content = f"""
ì œëª©: {recall.get('title', '')}
ì¹´í…Œê³ ë¦¬: {recall.get('category', '')}
ë“±ê¸‰: {recall.get('class', 'Unclassified')}
ë°œíš¨ì¼: {recall.get('effective_date', '')}
ìµœì¢… ì—…ë°ì´íŠ¸: {recall.get('last_updated', '')}

ë¦¬ì½œ ë‚´ìš©:
{chunk_content}
                """.strip()
                
                # ë©”íƒ€ë°ì´í„° ì„¤ì • (realtime_crawlë¡œ ì¶œì²˜ í‘œì‹œ)
                metadata = {
                    "document_type": "recall",
                    "category": recall.get('category', ''),
                    "class": recall.get('class', 'Unclassified'),
                    "title": recall.get('title', ''),
                    "url": recall_url,
                    "effective_date": recall.get('effective_date', ''),
                    "last_updated": recall.get('last_updated', ''),
                    "chunk_index": str(i),
                    "source": "realtime_crawl",  # ì‹¤ì‹œê°„ í¬ë¡¤ë§ í‘œì‹œ
                    "crawl_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # í¬ë¡¤ë§ ì‹œê°„ ì¶”ê°€
                }
                
                doc = Document(page_content=structured_content, metadata=metadata)
                new_documents.append(doc)
        
        # 3. ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
        if new_documents:
            try:
                # ChromaëŠ” add_documents ë©”ì„œë“œ ì‚¬ìš©
                vectorstore.add_documents(new_documents)
                print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ì— {len(new_documents)}ê°œ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
                
                # ì¶”ê°€ í›„ ìƒíƒœ í™•ì¸
                total_count = vectorstore._collection.count()
                print(f"ğŸ“Š í˜„ì¬ ë²¡í„°ìŠ¤í† ì–´ ì´ ë¬¸ì„œ ìˆ˜: {total_count}ê°œ")
                
            except Exception as e:
                print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì¶”ê°€ ì˜¤ë¥˜: {e}")
                return 0
        else:
            print("â„¹ï¸ ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ì¤‘ë³µ ë˜ëŠ” ë¹ˆ ë‚´ìš©)")
            
        return len(new_documents)
        
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì—…ë°ì´íŠ¸ ì „ì²´ ì˜¤ë¥˜: {e}")
        return 0

# ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸ í•¨ìˆ˜ ì¶”ê°€
def check_vectorstore_status(vectorstore=None) -> Dict[str, Any]:
    """ë²¡í„°ìŠ¤í† ì–´ í˜„ì¬ ìƒíƒœ í™•ì¸"""
    if vectorstore is None:
        from utils.chat_recall import recall_vectorstore
        vectorstore = recall_vectorstore
    
    if vectorstore is None:
        return {
            "status": "disconnected",
            "error": "ë²¡í„°ìŠ¤í† ì–´ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }
    
    try:
        # ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        collection_data = vectorstore.get()
        metadatas = collection_data.get('metadatas', [])
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ì¹´ìš´íŠ¸
        realtime_count = sum(1 for m in metadatas if m and m.get('source') == 'realtime_crawl')
        total_count = len(metadatas)
        
        # ìµœê·¼ í¬ë¡¤ë§ ì‹œê°„
        recent_crawl = None
        for metadata in metadatas:
            if metadata and metadata.get('source') == 'realtime_crawl':
                crawl_time = metadata.get('crawl_timestamp')
                if crawl_time and (recent_crawl is None or crawl_time > recent_crawl):
                    recent_crawl = crawl_time
        
        return {
            "status": "connected",
            "total_documents": total_count,
            "realtime_documents": realtime_count,
            "database_documents": total_count - realtime_count,
            "realtime_ratio": (realtime_count / total_count * 100) if total_count > 0 else 0,
            "recent_crawl_time": recent_crawl,
            "vectorstore_path": vectorstore._persist_directory if hasattr(vectorstore, '_persist_directory') else 'Unknown'
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": f"ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}"
        }
    
def get_latest_crawl_time(df):
    """ìµœê·¼ í¬ë¡¤ë§ ì‹œê°„ ë°˜í™˜"""
    try:
        realtime_df = df[df['is_realtime'] == True]
        if realtime_df.empty:
            return "ì—†ìŒ"
        
        crawl_times = realtime_df['crawl_timestamp'].dropna()
        if crawl_times.empty:
            return "ì—†ìŒ"
        
        latest_time = crawl_times.max()
        return latest_time if latest_time else "ì—†ìŒ"
        
    except Exception:
        return "ì—†ìŒ"
    
    
# ë‚˜ë¨¸ì§€ ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
def create_recall_visualizations(vectorstore) -> Dict[str, Any]:
    """ë¦¬ì½œ ë°ì´í„° ì‹œê°í™” ìƒì„± - ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë¶„ í‘œì‹œ"""
    try:
        all_data = vectorstore.get()
        metadatas = all_data.get('metadatas', [])
        documents = all_data.get('documents', [])
        
        if not metadatas:
            return {}
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë¶„)
        df_data = []
        for i, metadata in enumerate(metadatas):
            if metadata:
                doc_content = documents[i] if i < len(documents) else ""
                is_realtime = metadata.get('source') == 'realtime_crawl'
                
                df_data.append({
                    'category': metadata.get('category', 'Other') or 'Other',
                    'class': metadata.get('class', 'Unclassified') or 'Unclassified',
                    'effective_date': metadata.get('effective_date', '') or '',
                    'source': metadata.get('source', 'unknown') or 'unknown',
                    'is_realtime': is_realtime,  # ì‹¤ì‹œê°„ ë°ì´í„° ì—¬ë¶€
                    'title': metadata.get('title', '') or '',
                    'content': doc_content[:500],
                    'url': metadata.get('url', '') or '',
                    'crawl_timestamp': metadata.get('crawl_timestamp', '') or ''
                })
        
        if not df_data:
            return {}
            
        df = pd.DataFrame(df_data)
        
        # ì‹¤ì‹œê°„ ë°ì´í„° í†µê³„
        realtime_count = len(df[df['is_realtime'] == True])
        total_count = len(df)
        database_count = total_count - realtime_count
        
        # í†µê³„ ìš”ì•½ (ì‹¤ì‹œê°„ ë°ì´í„° ì •ë³´ ì¶”ê°€)
        stats = {
            'total_recalls': total_count,
            'realtime_recalls': realtime_count,
            'database_recalls': database_count,
            'realtime_ratio': (realtime_count / total_count * 100) if total_count > 0 else 0,
            'date_range': get_date_range(df),
            'avg_monthly': calculate_monthly_average(df),
            'peak_month': get_peak_month(df),
            'latest_crawl': get_latest_crawl_time(df)
        }
        
        return {
            'stats': stats,
            'dataframe': df.drop(['content'], axis=1)
        }
        
    except Exception as e:
        print(f"ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
        return {}

def get_date_range(df):
    """ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
    try:
        dates = pd.to_datetime(df['effective_date'], errors='coerce').dropna()
        if dates.empty:
            return "N/A"
        return f"{dates.min().strftime('%Y-%m')} ~ {dates.max().strftime('%Y-%m')}"
    except:
        return "N/A"

def calculate_monthly_average(df):
    """ì›”í‰ê·  ê³„ì‚°"""
    try:
        dates = pd.to_datetime(df['effective_date'], errors='coerce').dropna()
        if dates.empty:
            return 0
        months = (dates.max() - dates.min()).days / 30.44
        return round(len(df) / max(1, months), 1)
    except:
        return 0

def get_peak_month(df):
    """í”¼í¬ ì›” ê³„ì‚°"""
    try:
        df['datetime'] = pd.to_datetime(df['effective_date'], errors='coerce')
        valid_dates = df[df['datetime'].notna()]
        if valid_dates.empty:
            return "N/A"
        peak_month = valid_dates['datetime'].dt.month.mode().iloc[0]
        month_names = ['', '1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”', 
                      '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”']
        return month_names[peak_month]
    except:
        return "N/A"

# ìºì‹œëœ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_crawler():
    return FDARealtimeCrawler()

def perform_realtime_update(vectorstore=None, days_back: int = 3) -> Dict[str, Any]:
    """ì‹¤ì‹œê°„ ë¦¬ì½œ ë°ì´í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰ - ì™„ì „ êµ¬í˜„ ë²„ì „"""
    try:
        # vectorstoreê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œì—ì„œ ë¡œë“œ
        if vectorstore is None:
            from utils.chat_recall import recall_vectorstore
            vectorstore = recall_vectorstore
            
        if vectorstore is None:
            return {
                'success': False,
                'error': 'vectorstore not available',
                'message': "ë²¡í„°ìŠ¤í† ì–´ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # 1. ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìˆ˜í–‰
        crawler = get_crawler()
        
        print(f"ğŸ” ìµœê·¼ {days_back}ì¼ê°„ì˜ ë¦¬ì½œ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
        new_recalls = crawler.crawl_latest_recalls(days_back=days_back)
        
        if not new_recalls:
            return {
                'success': True,
                'crawled_count': 0,
                'added_count': 0,
                'message': "ìƒˆë¡œìš´ ë¦¬ì½œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(new_recalls)}ê±´ ë°œê²¬")
        
        # 2. ë²¡í„°ìŠ¤í† ì–´ì— ë°ì´í„° ì¶”ê°€
        print("ğŸ“¥ ë²¡í„°ìŠ¤í† ì–´ì— ìƒˆ ë°ì´í„° ì¶”ê°€ ì¤‘...")
        added_count = update_vectorstore_with_new_data(new_recalls, vectorstore)
        
        # 3. ê²°ê³¼ ë°˜í™˜
        return {
            'success': True,
            'crawled_count': len(new_recalls),
            'added_count': added_count,
            'new_recalls': new_recalls,  # í¬ë¡¤ë§ëœ ì›ë³¸ ë°ì´í„°
            'message': f"ìƒˆë¡œìš´ ë¦¬ì½œ {len(new_recalls)}ê±´ ë°œê²¬, {added_count}ê±´ì˜ ë¬¸ì„œ ì¶”ê°€ë¨"
        }
        
    except Exception as e:
        print(f"âŒ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return {
            'success': False,
            'error': str(e),
            'crawled_count': 0,
            'added_count': 0,
            'message': f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}"
        }
