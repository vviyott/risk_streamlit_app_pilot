# utils/chat_recall.py - ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™ ë²„ì „

# sqlite ì˜¤ë¥˜ ìš°íšŒìš© (ChromaDB + Streamlit Cloud)
import sys
import pysqlite3

sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import json
import os
from typing import TypedDict, List, Dict, Any, Optional
from functools import wraps
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_community.tools import DuckDuckGoSearchRun 
from langchain.tools import Tool
from langgraph.graph import StateGraph, START, END
from langchain_teddynote import logging # LangSmith ì¶”ì  í™œì„±í™”

load_dotenv() # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
logging.langsmith("LLMPROJECT") # LangSmith ì¶”ì  ì„¤ì •

class RecallState(TypedDict):
    """ë¦¬ì½œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒíƒœ"""
    question: str
    question_en: str
    recall_context: str
    recall_documents: List[Document]
    web_search_results: str
    final_answer: str
    search_method: str  # "recall_only", "web_only", "hybrid"
    needs_web_search: bool
    chat_history: List[HumanMessage | AIMessage]

def translate_to_english(korean_text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        prompt = f"Translate the following Korean text to English. Only return the translation:\n\n{korean_text}"
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return korean_text

def load_recall_documents():
    """FDA ë¦¬ì½œ ë°ì´í„° ë¡œë“œ - JSON êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •"""
    recall_file = "fda_recall.json"
    documents = []
    
    try:
        with open(recall_file, "r", encoding="utf-8") as f:
            recall_data = json.load(f)
            
            for item in recall_data:
                if isinstance(item, dict) and item.get("document_type") == "recall":
                    
                    # chunksë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬
                    chunks = item.get("chunks", [])
                    for i, chunk_content in enumerate(chunks):
                        
                        # ë¹ˆ ë‚´ìš© ê±´ë„ˆë›°ê¸°
                        if not chunk_content or len(chunk_content.strip()) < 30:
                            continue
                        
                        # êµ¬ì¡°í™”ëœ ì»¨í…ì¸  ìƒì„±
                        structured_content = f"""
ì œëª©: {item.get('title', '')}
ì¹´í…Œê³ ë¦¬: {item.get('category', '')}
ë“±ê¸‰: {item.get('class', 'Unclassified')}
ë°œíš¨ì¼: {item.get('effective_date', '')}
ìµœì¢… ì—…ë°ì´íŠ¸: {item.get('last_updated', '')}

ë¦¬ì½œ ë‚´ìš©:
{chunk_content}
                        """.strip()
                        
                        # ë©”íƒ€ë°ì´í„° ìƒì„± - ğŸ†• class í•„ë“œ ì¶”ê°€
                        metadata = {
                            "document_type": item.get("document_type", ""),
                            "category": item.get("category", ""),
                            "class": item.get("class", "Unclassified"),  # ğŸ†• ì¶”ê°€
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "effective_date": item.get("effective_date", ""),
                            "last_updated": item.get("last_updated", ""),
                            "chunk_index": str(i),
                            "source": "fda_recall_database"  # ğŸ†• ì¶œì²˜ í‘œì‹œ
                        }
                        
                        doc = Document(page_content=structured_content, metadata=metadata)
                        documents.append(doc)
        
        print(f"ë¦¬ì½œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ì²­í¬")
        return documents
        
    except FileNotFoundError:
        print(f"ë¦¬ì½œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {recall_file}")
        return []
    except Exception as e:
        print(f"ë¦¬ì½œ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []
    
# ì›¹ ê²€ìƒ‰ ë˜í¼ ì´ˆê¸°í™” (ë” ì•ˆì •ì )
search_wrapper = DuckDuckGoSearchAPIWrapper(
    region="us-en",  # ë¯¸êµ­ ì˜ì–´ë¡œ ê²€ìƒ‰
    time="y",        # ìµœê·¼ 1ë…„ ê²°ê³¼ ìš°ì„ 
    max_results=3    # ê²°ê³¼ ê°œìˆ˜ ì œí•œ
)

def web_search_tool(query: str) -> str:
    """ì•ˆì •ì ì¸ ì›¹ ê²€ìƒ‰ í•¨ìˆ˜"""
    try:
        results = search_wrapper.run(query)
        return results
    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def initialize_recall_vectorstore():
    """ë¦¬ì½œ ì „ìš© ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ì§€ì›"""
    persist_dir = "./data/chroma_db_recall"
    
    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        try:
            print("ê¸°ì¡´ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings,
                collection_name="FDA_recalls"
            )
            
            collection = vectorstore._collection
            if collection.count() > 0:
                print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ ({collection.count()}ê°œ ë¬¸ì„œ)")
                
                # ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ë¹„ìœ¨ ì²´í¬
                try:
                    all_data = vectorstore.get()
                    metadatas = all_data.get('metadatas', [])
                    realtime_count = sum(1 for m in metadatas if m and m.get('source') == 'realtime_crawl')
                    total_count = len(metadatas)
                    print(f"ì‹¤ì‹œê°„ ë°ì´í„°: {realtime_count}/{total_count}ê±´")
                except:
                    pass
                
                return vectorstore
                
        except Exception as e:
            print(f"ê¸°ì¡´ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    try:
        print("ìƒˆ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        documents = load_recall_documents()
        
        if not documents:
            raise ValueError("ë¡œë“œëœ ë¦¬ì½œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name="FDA_recalls",
            persist_directory=persist_dir
        )
        
        print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({len(documents)}ê°œ ë¬¸ì„œ)")
        return vectorstore
        
    except Exception as e:
        print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        raise

# ì „ì—­ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
try:
    recall_vectorstore = initialize_recall_vectorstore()
except Exception as e:
    print(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    recall_vectorstore = None

# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
web_search = DuckDuckGoSearchRun()

def translation_node(state: RecallState) -> RecallState:
    """ë²ˆì—­ ë…¸ë“œ"""
    question_en = translate_to_english(state["question"])
    
    return {
        **state,
        "question_en": question_en
    }

def recall_search_node(state: RecallState) -> RecallState:
    """ë¦¬ì½œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ + ìµœì í™”ëœ ì‹¤ì‹œê°„ í¬ë¡¤ë§"""
    if recall_vectorstore is None:
        print("ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            **state,
            "recall_context": "",
            "recall_documents": [],
            "needs_web_search": True
        }
    
    try:
        # ğŸ†• ìµœì í™”ëœ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìˆ˜í–‰ (ë²¡í„°ìŠ¤í† ì–´ ì „ë‹¬)
        from utils.fda_realtime_crawler import get_crawler, update_vectorstore_with_new_data
        
        print("ğŸ” ë²¡í„°DB ê¸°ë°˜ ìµœì í™”ëœ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì¤‘...")
        crawler = get_crawler()
        
        # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì „ë‹¬í•˜ì—¬ ì¤‘ë³µ ì²´í¬ í›„ í¬ë¡¤ë§
        new_recalls = crawler.crawl_latest_recalls(days_back=15, vectorstore=recall_vectorstore)
        
        if new_recalls:
            added_count = update_vectorstore_with_new_data(new_recalls, recall_vectorstore)
            print(f"âœ… ìƒˆ ë°ì´í„° {len(new_recalls)}ê±´ í¬ë¡¤ë§, {added_count}ê±´ ì¶”ê°€ë¨")
        else:
            print("ğŸ’¡ ìƒˆë¡œìš´ Food & Beverages ë¦¬ì½œ ì—†ìŒ")
        
        # ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§ ìˆ˜í–‰
        retriever = recall_vectorstore.as_retriever(search_kwargs={"k": 8})
        
        korean_docs = retriever.invoke(state["question"])
        english_docs = retriever.invoke(state["question_en"])
        
        all_docs = korean_docs + english_docs
        unique_docs = []
        seen_content = set()
        
        for doc in all_docs:
            content_key = doc.page_content[:100]
            if content_key not in seen_content:
                unique_docs.append(doc)
                seen_content.add(content_key)
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„  ì •ë ¬
        def prioritize_docs(doc):
            priority_score = 0
            # ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„ 
            if doc.metadata.get("source") == "realtime_crawl":
                priority_score += 1000
            # ë‚ ì§œ ê¸°ì¤€ ìš°ì„ ìˆœìœ„
            try:
                date_str = doc.metadata.get("effective_date", "1900-01-01")
                if len(date_str) >= 10:
                    year = int(date_str[:4])
                    month = int(date_str[5:7])
                    priority_score += year * 100 + month
            except:
                pass
            return priority_score
        
        unique_docs.sort(key=prioritize_docs, reverse=True)
        selected_docs = unique_docs[:6]  # ë¬¸ì„œ ìˆ˜ ì¦ê°€
        context = "\n\n".join([doc.page_content for doc in selected_docs])
        
        # ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì›¹ ê²€ìƒ‰ ìµœì†Œí™”
        realtime_docs_selected = [doc for doc in selected_docs if doc.metadata.get("source") == "realtime_crawl"]
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ 2ê°œ ì´ìƒì´ë©´ ì›¹ ê²€ìƒ‰ ìƒëµ
        needs_web_search = len(selected_docs) < 3 or len(context) < 300
        if len(realtime_docs_selected) >= 2:
            needs_web_search = False
        elif len(realtime_docs_selected) >= 1 and len(context) > 500:
            needs_web_search = False
        
        print(f"ğŸ“Š ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(selected_docs)}ê±´ (âš¡ì‹¤ì‹œê°„: {len(realtime_docs_selected)}ê±´, ğŸ“šê¸°ì¡´: {len(selected_docs) - len(realtime_docs_selected)}ê±´)")
        print(f"ğŸŒ ì›¹ ê²€ìƒ‰ í•„ìš”: {needs_web_search}")
        
        return {
            **state,
            "recall_context": context,
            "recall_documents": selected_docs,
            "needs_web_search": needs_web_search
        }
        
    except Exception as e:
        print(f"âŒ ìë™ í¬ë¡¤ë§ + ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "recall_context": "",
            "recall_documents": [],
            "needs_web_search": True
        }

def web_search_node(state: RecallState) -> RecallState:
    """ê°œì„ ëœ ì›¹ ê²€ìƒ‰ ë…¸ë“œ - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ìˆì„ ë•Œ ì œí•œì  ê²€ìƒ‰"""
    if not state["needs_web_search"]:
        return {
            **state,
            "web_search_results": "",
            "search_method": "recall_only"
        }
    
    try:
        # ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°„ë‹¨í•œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        realtime_docs = [doc for doc in state["recall_documents"] 
                        if doc.metadata.get("source") == "realtime_crawl"]
        
        if realtime_docs:
            # ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì œí•œì  ì›¹ ê²€ìƒ‰
            search_queries = [f"FDA recall {state['question_en']}"]
            print("ì‹¤ì‹œê°„ ë°ì´í„° ì¡´ì¬ - ì œí•œì  ì›¹ ê²€ìƒ‰ ìˆ˜í–‰")
        else:
            # ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì›¹ ê²€ìƒ‰
            search_queries = [
                f"FDA recall {state['question_en']}",
                f"food safety recall {state['question_en']}",
                f"{state['question_en']} recall 2024 2025"
            ]
            print("ì‹¤ì‹œê°„ ë°ì´í„° ì—†ìŒ - ì „ì²´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰")
        
        all_results = []
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        for query in search_queries:
            try:
                result = web_search_tool(query)
                if result and "ì˜¤ë¥˜" not in result:
                    all_results.append(f"[ê²€ìƒ‰ì–´: {query}]\n{result}")
                    if realtime_docs:  # ì‹¤ì‹œê°„ ë°ì´í„° ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ
                        break
            except Exception as e:
                print(f"ê²€ìƒ‰ì–´ '{query}' ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ê²°í•©
        web_results = "\n\n".join(all_results) if all_results else "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê²€ìƒ‰ ë°©ë²• ê²°ì •
        if not state["recall_context"]:
            search_method = "web_only"
        else:
            search_method = "hybrid"
        
        print(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(web_results)}ì")
        
        return {
            **state,
            "web_search_results": web_results,
            "search_method": search_method
        }
        
    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ì „ì²´ ì‹¤íŒ¨: {e}")
        search_method = "recall_only" if state["recall_context"] else "error"
        
        return {
            **state,
            "web_search_results": f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}",
            "search_method": search_method
        }

# ============= í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒìˆ˜ë“¤ - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„  ë°˜ì˜ =============

# 1) FDA ë¦¬ì½œ DBë§Œìœ¼ë¡œ ë‹µë³€
PROMPT_RECALL_ONLY = """
ë‹¹ì‹ ì€ **FDA ë¦¬ì½œÂ·íšŒìˆ˜(Recalls, Market Withdrawals, Safety Alerts) ì „ë¬¸ ë¶„ì„ê°€**ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë¦¬ì½œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.  

ğŸ“Œ ì‘ì„± ê·œì¹™  
1. ë°˜ë“œì‹œ ì œê³µëœ "FDA Recall Database Information"ë§Œ ê·¼ê±°ë¡œ ì‚¼ìŠµë‹ˆë‹¤.  
2. **ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„°(realtime_crawl)ê°€ í¬í•¨ëœ ê²½ìš° ìš°ì„  ì°¸ê³ **í•˜ì—¬ ìµœì‹ ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
3. **ë¦¬ì½œ ì‚¬ë¡€ê°€ 1ê±´ ì´ìƒ**ì´ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.  
   | ë‚ ì§œ | ë¸Œëœë“œ | ì œí’ˆ | ë¦¬ì½œ ì‚¬ìœ  | ë“±ê¸‰ | ì¢…ë£Œ ì—¬ë¶€ | ì¶œì²˜ |  
4. **ì¶œì²˜ ë§í¬**ê°€ ìˆìœ¼ë©´ ì…€ì— í•˜ì´í¼ë§í¬ í˜•íƒœë¡œ ë„£ìŠµë‹ˆë‹¤.  
5. **ì „í˜€ ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ë§Œ ìˆê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ** "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ í•´ë‹¹ ì‚¬ë¡€ í™•ì¸ ë¶ˆê°€"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”. ì¡°ê¸ˆì´ë¼ë„ ê´€ë ¨ëœ ë¦¬ì½œ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
6. ëª¨ë“  í‘œ ì•„ë˜ì— 3â€“5ë¬¸ì¥ ê·œëª¨ë¡œ **ì¢…í•© ìš”ì•½**(ê¸°ì—… ê´€ì ì—ì„œ ìœ„í—˜ë„Â·ì˜ˆë°©ì¡°ì¹˜Â·ì¤€ìˆ˜ì‚¬í•­ ë“±) ì„ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
7. **ğŸ†• ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš° "âš¡ ìµœì‹  ì—…ë°ì´íŠ¸ í¬í•¨" í‘œì‹œ**ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

ğŸ“ ì§ˆë¬¸:  
{question}

ğŸ“’ FDA Recall Database Information:  
{recall_context}

ğŸ”½ ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
"""

# 2) ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œìœ¼ë¡œ ë‹µë³€ (ê¸°ì¡´ê³¼ ë™ì¼)
PROMPT_WEB_ONLY = """
ì—­í• : FDA ë¦¬ì½œ ë¶„ì„ê°€  
ì¡°ê±´: ì•„ë˜ "Web Search Results"ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ ë¸Œë¦¬í•‘ ì‘ì„±  
ê·œì¹™Â·í˜•ì‹ì€ PROMPT_RECALL_ONLYì™€ ë™ì¼í•˜ê²Œ ì ìš©  

ğŸ“Œ ì‘ì„± ê·œì¹™  
1. ë°˜ë“œì‹œ ì œê³µëœ "Web Search Results"ë§Œ ê·¼ê±°ë¡œ ì‚¼ìŠµë‹ˆë‹¤.  
2. **ë¦¬ì½œ ì‚¬ë¡€ê°€ 1ê±´ ì´ìƒ**ì´ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.  
   | ë‚ ì§œ | ë¸Œëœë“œ | ì œí’ˆ | ë¦¬ì½œ ì‚¬ìœ  | ë“±ê¸‰ | ì¢…ë£Œ ì—¬ë¶€ | ì›ë¬¸ ë§í¬ |  
3. **ì¶œì²˜ ë§í¬**ê°€ ìˆìœ¼ë©´ ì…€ì— í•˜ì´í¼ë§í¬ í˜•íƒœë¡œ ë„£ìŠµë‹ˆë‹¤.  
4. **ì „í˜€ ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ë§Œ ìˆê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ** "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ í•´ë‹¹ ì‚¬ë¡€ í™•ì¸ ë¶ˆê°€"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”. ì¡°ê¸ˆì´ë¼ë„ ê´€ë ¨ëœ ë¦¬ì½œ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
5. ëª¨ë“  í‘œ ì•„ë˜ì— 3â€“5ë¬¸ì¥ ê·œëª¨ë¡œ **ì¢…í•© ìš”ì•½**(ê¸°ì—… ê´€ì ì—ì„œ ìœ„í—˜ë„Â·ì˜ˆë°©ì¡°ì¹˜Â·ì¤€ìˆ˜ì‚¬í•­ ë“±) ì„ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

ğŸ“ ì§ˆë¬¸:  
{question}

ğŸŒ Web Search Results:  
{web_results}

ğŸ”½ ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
"""

# 3) DB + ì›¹ì„ í•¨ê»˜ í™œìš© - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„  ê°•ì¡°
PROMPT_HYBRID = """
ì—­í• : FDA ë¦¬ì½œ ë¶„ì„ê°€  
ìë£Œ: "FDA Recall Database Information" ìš°ì„  â†’ ë¶€ì¡±í•˜ë©´ "Additional Web Search Results" ì°¸ê³   
ğŸ†• **ì‹¤ì‹œê°„ í¬ë¡¤ë§ ë°ì´í„°(realtime_crawl)ê°€ ìˆìœ¼ë©´ ìµœìš°ì„  ë°˜ì˜**  

ğŸ“Œ ì‘ì„± ê·œì¹™  
1. "FDA Recall Database Information"ì„ ìš°ì„  ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ê³ , ë¶€ì¡±í•œ ì •ë³´ëŠ” "Additional Web Search Results"ë¡œ ë³´ì™„í•©ë‹ˆë‹¤.  
2. **ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš° í•´ë‹¹ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜**í•˜ê³  í‘œì—ì„œ êµ¬ë¶„ í‘œì‹œí•©ë‹ˆë‹¤.
3. **ë¦¬ì½œ ì‚¬ë¡€ê°€ 1ê±´ ì´ìƒ**ì´ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.  
   | ë‚ ì§œ | ë¸Œëœë“œ | ì œí’ˆ | ë¦¬ì½œ ì‚¬ìœ  | ë“±ê¸‰ | ì¢…ë£Œ ì—¬ë¶€ | ì¶œì²˜ | ğŸ†•ì—…ë°ì´íŠ¸ |  
4. **ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” "âš¡ìµœì‹ " ë§ˆí¬**ë¥¼ ì¶”ê°€í•˜ì—¬ êµ¬ë¶„í•©ë‹ˆë‹¤.
5. **ì¶œì²˜ ë§í¬**ê°€ ìˆìœ¼ë©´ ì…€ì— í•˜ì´í¼ë§í¬ í˜•íƒœë¡œ ë„£ìŠµë‹ˆë‹¤.  
6. **ì „í˜€ ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ë§Œ ìˆê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ** "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ í•´ë‹¹ ì‚¬ë¡€ í™•ì¸ ë¶ˆê°€"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”. ì¡°ê¸ˆì´ë¼ë„ ê´€ë ¨ëœ ë¦¬ì½œ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
7. ëª¨ë“  í‘œ ì•„ë˜ì— 3â€“5ë¬¸ì¥ ê·œëª¨ë¡œ **ì¢…í•© ìš”ì•½**(ê¸°ì—… ê´€ì ì—ì„œ ìœ„í—˜ë„Â·ì˜ˆë°©ì¡°ì¹˜Â·ì¤€ìˆ˜ì‚¬í•­ ë“±) ì„ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
8. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš° "âš¡ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í¬í•¨" ì•ˆë‚´**ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

ğŸ“ ì§ˆë¬¸:  
{question}

ğŸ“’ FDA Recall Database Information:  
{recall_context}

ğŸŒ Additional Web Search Results:  
{web_results}

ğŸ”½ ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
"""

def answer_generation_node(state: RecallState) -> RecallState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„  ì²˜ë¦¬"""
    
    # ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
    realtime_docs = [doc for doc in state["recall_documents"] 
                    if doc.metadata.get("source") == "realtime_crawl"]
    has_realtime_data = len(realtime_docs) > 0
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ë³€ìˆ˜ ì„ íƒ
    if state["search_method"] == "recall_only":
        prompt_template = PROMPT_RECALL_ONLY
        prompt_vars = {
            "question": state["question"],
            "recall_context": state["recall_context"]
        }
        
    elif state["search_method"] == "web_only":
        prompt_template = PROMPT_WEB_ONLY
        prompt_vars = {
            "question": state["question"],
            "web_results": state["web_search_results"]
        }
        
    else:  # hybrid
        prompt_template = PROMPT_HYBRID
        prompt_vars = {
            "question": state["question"],
            "recall_context": state["recall_context"],
            "web_results": state["web_search_results"]
        }
    
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        prompt = PromptTemplate.from_template(prompt_template)
        chain = prompt | llm | StrOutputParser()
        
        answer = chain.invoke(prompt_vars)
        
        # ğŸ†• ê²€ìƒ‰ ì •ë³´ ì¶”ê°€ - ì‹¤ì‹œê°„ ë°ì´í„° ì •ë³´ í¬í•¨
        search_info = f"\n\nğŸ” ê²€ìƒ‰ ë°©ë²•: {state['search_method']}"
        
        if state["recall_documents"]:
            # ì‹¤ì‹œê°„ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„° êµ¬ë¶„
            realtime_count = len(realtime_docs)
            total_count = len(state["recall_documents"])
            
            search_info += f"\nğŸ“‹ ì°¸ì¡° ë¬¸ì„œ: ì´ {total_count}ê±´"
            if realtime_count > 0:
                search_info += f" (âš¡ì‹¤ì‹œê°„: {realtime_count}ê±´, ğŸ“šê¸°ì¡´: {total_count - realtime_count}ê±´)"
            
            # ë¦¬ì½œ ì œëª©ë“¤ ì¶”ì¶œ (ì‹¤ì‹œê°„ ë°ì´í„° ìš°ì„ )
            if realtime_docs:
                realtime_titles = [doc.metadata.get("title", "")[:50] + "..." 
                                 for doc in realtime_docs[:2] if doc.metadata.get("title")]
                if realtime_titles:
                    search_info += f"\nâš¡ ìµœì‹  ë¦¬ì½œ: {', '.join(realtime_titles)}"
            
            # ê¸°ì¡´ ë°ì´í„° ì œëª©
            existing_docs = [doc for doc in state["recall_documents"] 
                           if doc.metadata.get("source") != "realtime_crawl"]
            if existing_docs and not realtime_docs:  # ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì—†ì„ ë•Œë§Œ í‘œì‹œ
                existing_titles = [doc.metadata.get("title", "")[:50] + "..." 
                                 for doc in existing_docs[:2] if doc.metadata.get("title")]
                if existing_titles:
                    search_info += f"\nğŸ“š ê¸°ì¡´ ì‚¬ë¡€: {', '.join(existing_titles)}"
        
        # ğŸ†• ë°ì´í„° ì‹ ì„ ë„ í‘œì‹œ
        if has_realtime_data:
            search_info += f"\nâœ¨ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë°ì´í„° í¬í•¨"
        
        final_answer = f"{answer}{search_info}"
        
        return {
            **state,
            "final_answer": final_answer
        }
        
    except Exception as e:
        error_answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        return {
            **state,
            "final_answer": error_answer
        }

def update_history_node(state: RecallState) -> RecallState:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
    try:
        current_history = state.get("chat_history", [])
        
        updated_history = current_history.copy()
        updated_history.append(HumanMessage(content=state["question"]))
        updated_history.append(AIMessage(content=state["final_answer"]))
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 8ê°œ ë©”ì‹œì§€)
        if len(updated_history) > 8:
            updated_history = updated_history[-8:]
        
        return {
            **state,
            "chat_history": updated_history
        }
        
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return state

# ê·¸ë˜í”„ êµ¬ì„±
recall_workflow = StateGraph(RecallState)

# ë…¸ë“œ ì¶”ê°€
recall_workflow.add_node("translate", translation_node)
recall_workflow.add_node("recall_search", recall_search_node)
recall_workflow.add_node("web_search", web_search_node)
recall_workflow.add_node("generate_answer", answer_generation_node)
recall_workflow.add_node("update_history", update_history_node)

# ì—£ì§€ ì¶”ê°€
recall_workflow.add_edge(START, "translate")
recall_workflow.add_edge("translate", "recall_search")
recall_workflow.add_edge("recall_search", "web_search")
recall_workflow.add_edge("web_search", "generate_answer")
recall_workflow.add_edge("generate_answer", "update_history")
recall_workflow.add_edge("update_history", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
recall_graph = recall_workflow.compile()

def ask_recall_question(question: str, chat_history: List = None) -> Dict[str, Any]:
    """ë¦¬ì½œ ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ - ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° ì§€ì›"""
    if chat_history is None:
        chat_history = []
    
    try:
        result = recall_graph.invoke({
            "question": question,
            "question_en": "",
            "recall_context": "",
            "recall_documents": [],
            "web_search_results": "",
            "final_answer": "",
            "search_method": "",
            "needs_web_search": False,
            "chat_history": chat_history
        })
        
        # ğŸ†• ì‹¤ì‹œê°„ ë°ì´í„° í¬í•¨ ì—¬ë¶€ ì •ë³´ ì¶”ê°€
        realtime_docs = [doc for doc in result["recall_documents"] 
                        if doc.metadata.get("source") == "realtime_crawl"]
        
        return {
            "answer": result["final_answer"],
            "search_method": result["search_method"],
            "recall_documents": result["recall_documents"],
            "chat_history": result["chat_history"],
            "has_realtime_data": len(realtime_docs) > 0,  # ğŸ†• ì¶”ê°€
            "realtime_count": len(realtime_docs),  # ğŸ†• ì¶”ê°€
            "total_documents": len(result["recall_documents"])  # ğŸ†• ì¶”ê°€
        }
        
    except Exception as e:
        return {
            "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "search_method": "error",
            "recall_documents": [],
            "chat_history": chat_history,
            "has_realtime_data": False,  # ğŸ†• ì¶”ê°€
            "realtime_count": 0,  # ğŸ†• ì¶”ê°€
            "total_documents": 0  # ğŸ†• ì¶”ê°€
        }

# ğŸ†• ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ ì²´í¬ í•¨ìˆ˜
def get_vectorstore_status() -> Dict[str, Any]:
    """ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    if recall_vectorstore is None:
        return {
            "status": "disconnected",
            "total_documents": 0,
            "realtime_documents": 0,
            "error": "ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        }
    
    try:
        collection_data = recall_vectorstore.get()
        metadatas = collection_data.get('metadatas', [])
        
        total_docs = len(metadatas)
        realtime_docs = sum(1 for m in metadatas if m and m.get('source') == 'realtime_crawl')
        
        return {
            "status": "connected",
            "total_documents": total_docs,
            "realtime_documents": realtime_docs,
            "realtime_ratio": (realtime_docs / total_docs * 100) if total_docs > 0 else 0,
            "categories": len(set(m.get('category', 'Other') for m in metadatas if m)),
            "last_updated": max([m.get('last_updated', '') for m in metadatas if m], default='Unknown')
        }
        
    except Exception as e:
        return {
            "status": "error",
            "total_documents": 0,
            "realtime_documents": 0,
            "error": str(e)
        }
