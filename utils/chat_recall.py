# utils/chat_recall.py - í•µì‹¬ ê¸°ëŠ¥ë§Œ ë‚¨ê¸´ ë²„ì „

import json
import os
from datetime import datetime, timedelta
from typing import TypedDict, List, Dict, Any
# from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langchain_teddynote import logging
from utils.fda_realtime_crawler import get_crawler, update_vectorstore_with_new_data,get_latest_date_from_vectorstore
from utils.google_crawler import search_and_extract_news, format_news_for_context

# load_dotenv()
logging.langsmith("LLMPROJECT")

class RecallState(TypedDict):
    """ë¦¬ì½œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒíƒœ"""
    question: str
    question_en: str  # ì˜ì–´ ë²ˆì—­ëœ ì§ˆë¬¸
    recall_context: str
    recall_documents: List[Document]
    final_answer: str
    chat_history: List[HumanMessage | AIMessage]
    news_context: str  # êµ¬ê¸€ ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    final_answer: str
    news_documents: List[Dict]  # ë‰´ìŠ¤ ë¬¸ì„œë“¤ ì¶”ê°€

def load_recall_documents():
    """ì´ ì½”ë“œëŠ” FDA ë¦¬ì½œ JSON ë°ì´í„°ë¥¼ ì²­í¬ ì—†ì´ ë‹¨ì¼ ë¬¸ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""
    recall_file = "fda_recall.json"
    documents = []
    
    try:
        with open(recall_file, "r", encoding="utf-8") as f:
            recall_data = json.load(f)
            
            for item in recall_data:
                if isinstance(item, dict) and item.get("document_type") == "recall":
                    # ğŸ†• ì²­í¬ë¥¼ í•˜ë‚˜ì˜ ì „ì²´ ë‚´ìš©ìœ¼ë¡œ ê²°í•©
                    chunks = item.get("chunks", [])
                    if not chunks:
                        continue
                    
                    # ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                    full_content = "\n\n".join(chunk for chunk in chunks if chunk and len(chunk.strip()) > 30)
                    
                    if not full_content or len(full_content.strip()) < 100:
                        continue
                    
                    # êµ¬ì¡°í™”ëœ ì»¨í…ì¸  ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
                    structured_content = f"""
ì œëª©: {item.get('title', '')}
ì¹´í…Œê³ ë¦¬: {item.get('category', '')}
ë“±ê¸‰: {item.get('class', 'Unclassified')}
ë°œíš¨ì¼: {item.get('effective_date', '')}
ìµœì¢… ì—…ë°ì´íŠ¸: {item.get('last_updated', '')}

ë¦¬ì½œ ë‚´ìš©:
{full_content}
                    """.strip()
                    
                    metadata = {
                        "document_type": "recall",
                        "category": item.get("category", ""),
                        "class": item.get("class", "Unclassified"),
                        "title": item.get("title", ""),
                        "url": item.get("url", ""),
                        "effective_date": item.get("effective_date", ""),
                        "source": "fda_recall_database"
                        # ğŸ†• chunk_index ì œê±° - ë” ì´ìƒ ì²­í¬ê°€ ì•„ë‹˜
                    }
                    
                    doc = Document(page_content=structured_content, metadata=metadata)
                    documents.append(doc)
        
        print(f"ë¦¬ì½œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ (ì²­í¬ ì œê±°)")
        return documents
        
    except FileNotFoundError:
        print(f"ë¦¬ì½œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {recall_file}")
        return []
    except Exception as e:
        print(f"ë¦¬ì½œ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def initialize_recall_vectorstore():
    """ì´ ì½”ë“œëŠ” ë¦¬ì½œ ì „ìš© ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
    persist_dir = "./data/chroma_db_recall"
    
    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        try:
            print("ê¸°ì¡´ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings,
                collection_name="FDA_recalls"
            )
            
            collection = vectorstore._collection
            if collection.count() > 0:
                print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ ({collection.count()}ê°œ ë¬¸ì„œ)")
                return vectorstore
                
        except Exception as e:
            print(f"ê¸°ì¡´ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    try:
        print("ìƒˆ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        documents = load_recall_documents()
        
        if not documents:
            raise ValueError("ë¡œë“œëœ ë¦¬ì½œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name="FDA_recalls",
            persist_directory=persist_dir
        )
        
        print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({len(documents)}ê°œ ë¬¸ì„œ)")
        return vectorstore
        
    except Exception as e:
        print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        raise

# ì „ì—­ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
try:
    recall_vectorstore = initialize_recall_vectorstore()
except Exception as e:
    print(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    recall_vectorstore = None

def translation_node(state: RecallState) -> RecallState:
    """ì¡°ê±´ë¶€ ë²ˆì—­ ë…¸ë“œ - ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­"""
    
    will_use_vectorstore = recall_vectorstore is not None
    
    if will_use_vectorstore:
        # ğŸ†• ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­ ìˆ˜í–‰
        question_en = translate_with_proper_nouns(state["question"])
        print(f"ğŸ”¤ ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­: '{state['question']}' â†’ '{question_en}'")
    else:
        question_en = state["question"]
        print(f"ğŸ”¤ ë²ˆì—­ ìƒëµ (ì›¹ ê²€ìƒ‰ ì „ìš©): '{question_en}'")
    
    # ğŸ†• ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
    search_keywords = extract_search_keywords(state["question"])
    
    return {
        **state,
        "question_en": question_en,
        "search_keywords": search_keywords  # ğŸ†• í‚¤ì›Œë“œ ì¶”ê°€
    }

def translate_with_proper_nouns(korean_text: str) -> str:
    """ê³ ìœ ëª…ì‚¬ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë²ˆì—­í•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        # ğŸ†• ê³ ìœ ëª…ì‚¬ ë³´ì¡´ í”„ë¡¬í”„íŠ¸
        prompt = f"""
            ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ë˜, ì œí’ˆëª…ê³¼ ë¸Œëœë“œëª…ì€ ì›í˜•ì„ ìœ ì§€í•˜ì„¸ìš”.
            
            ë²ˆì—­ ê·œì¹™:
            1. ì œí’ˆëª…/ë¸Œëœë“œëª…ì€ í•œêµ­ì–´ ì›í˜• ìœ ì§€ (ì˜ˆ: ë¶ˆë‹­ë³¶ìŒë©´ â†’ Buldak)
            2. ì¼ë°˜ì ì¸ ì‹í’ˆ ì¹´í…Œê³ ë¦¬ë§Œ ì˜ì–´ë¡œ ë²ˆì—­ (ì˜ˆ: ë¼ë©´ â†’ ramen, ê³¼ì â†’ snack)
            3. "ë¦¬ì½œ", "ì‚¬ë¡€" ë“±ì€ ì˜ì–´ë¡œ ë²ˆì—­
            4. ë²ˆì—­ë¬¸ë§Œ ë°˜í™˜í•˜ê³  ì„¤ëª… ì—†ì´
            
            ì˜ˆì‹œ:
            - "ë¶ˆë‹­ë³¶ìŒë©´ì˜ ë¦¬ì½œ ì‚¬ë¡€" â†’ "Buldak ramen recall case"
            - "ì˜¤ë¦¬ì˜¨ ì´ˆì½”íŒŒì´ ë¦¬ì½œ" â†’ "Orion Choco Pie recall"
            
            í•œêµ­ì–´ í…ìŠ¤íŠ¸: {korean_text}
            
            ì˜ì–´ ë²ˆì—­:"""

        response = llm.invoke([HumanMessage(content=prompt)])
        translated = response.content.strip()
        
        # ğŸ†• ë²ˆì—­ ê²°ê³¼ ê²€ì¦ ë° í›„ì²˜ë¦¬
        if translated and len(translated) > 0:
            # ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì œê±°
            translated = translated.replace('"', '').replace("'", "")
            if translated.lower().startswith('translation:'):
                translated = translated[12:].strip()
            return translated
        else:
            return korean_text
            
    except Exception as e:
        print(f"ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return korean_text
    
def extract_search_keywords(question: str) -> str:
    """ì´ ì½”ë“œëŠ” ì§ˆë¬¸ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        prompt = f"""
            ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ì í•©í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
            
            ê·œì¹™:
            1. ì œí’ˆëª…, ë¸Œëœë“œëª…, ì‹í’ˆëª…, íšŒì‚¬ëª…ë§Œ ì¶”ì¶œ
            2. "íšŒìˆ˜", "ì‚¬ë¡€", "ìˆë‚˜ìš”", "ì–´ë–¤", "ìµœê·¼", "ì–¸ì œ" ê°™ì€ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
            3. ì˜ì–´ ë¸Œëœë“œëª…ì€ ì›í˜• ìœ ì§€ (ì˜ˆ: McDonald's, KFC)
            4. ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œë¡œ ì œí•œ
            5. í‚¤ì›Œë“œ ë§ˆì§€ë§‰ì— "ë¦¬ì½œ" ë‹¨ì–´ í•­ìƒ ì¶”ê°€
            5. í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ë°˜í™˜ (ì„¤ëª…ì´ë‚˜ ë¶€ê°€ì„¤ëª… ì—†ì´)
            
            ì˜ˆì‹œ:
            - "ë§¥ë„ë‚ ë“œ í–„ë²„ê±° ë¦¬ì½œ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?" â†’ "ë§¥ë„ë‚ ë“œ í–„ë²„ê±° ë¦¬ì½œ"
            - "ì˜¤ë¦¬ì˜¨ ì´ˆì½”íŒŒì´ ìµœê·¼ ë¦¬ì½œ ì–´ë–¤ ê²Œ ìˆì–´?" â†’ "ì˜¤ë¦¬ì˜¨ ì´ˆì½”íŒŒì´ ë¦¬ì½œ"
            - "ë§Œë‘ ë¦¬ì½œ ì‚¬ë¡€" â†’ "ë§Œë‘ ë¦¬ì½œ"
            
            ì§ˆë¬¸: {question}
            í‚¤ì›Œë“œ:"""

        response = llm.invoke([HumanMessage(content=prompt)])
        keywords = response.content.strip()
        
        # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì œê±°
        keywords = keywords.replace('"', '').replace("'", "")
        if keywords.lower().startswith('í‚¤ì›Œë“œ:'):
            keywords = keywords[3:].strip()
        
        print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ: '{question}' â†’ '{keywords}'")
        return keywords if keywords else question
        
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        # fallback: ê°„ë‹¨í•œ ì •ê·œì‹ ë°©ì‹
        return extract_keywords_fallback(question)


def extract_keywords_fallback(question: str) -> str:
    """fallback í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹"""
    import re
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = ["íšŒìˆ˜", "ì‚¬ë¡€", "ìˆë‚˜ìš”", "ì–´ë–¤", "ì–´ë–»ê²Œ", "ì–¸ì œ", "ì™œ", "ìµœê·¼", "ìš”ì¦˜", "í˜„ì¬"]
    
    # í•œê¸€ê³¼ ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
    words = re.findall(r'[ê°€-í£A-Za-z]+', question)
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    result = " ".join(keywords[:3])
    print(f"ğŸ” Fallback í‚¤ì›Œë“œ: '{question}' â†’ '{result}'")
    return result if result else question
    

def is_recall_related_question(question: str) -> bool:
    """ì´ ì½”ë“œëŠ” ì§ˆë¬¸ì´ ë¦¬ì½œ ê´€ë ¨ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤"""
    recall_keywords = [
        "ë¦¬ì½œ", "íšŒìˆ˜", "recall", "withdrawal", "safety alert",
        "FDA", "ì‹í’ˆì•ˆì „", "ì œí’ˆ ë¬¸ì œ", "ì˜¤ì—¼", "contamination",
        "ì„¸ê· ", "bacteria", "E.coli", "salmonella", "listeria",
        "ì•Œë ˆë¥´ê¸°", "allergen", "ë¼ë²¨ë§", "labeling",
        "ì‹ì¤‘ë…", "ì•ˆì „", "ìœ„í—˜", "ë¬¸ì œ", "ì‚¬ê³ "
    ]
    
    question_lower = question.lower()
    return any(keyword.lower() in question_lower for keyword in recall_keywords)

def recall_search_node(state: RecallState) -> RecallState:
    """ì´ ì½”ë“œëŠ” ë²¡í„°DBì—ì„œ ë¦¬ì½œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ì‹¤ì‹œê°„ í¬ë¡¤ë§ì„ ì¡°ê±´ë¶€ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
    
    # ì¼ë°˜ ì§ˆë¬¸ì´ë©´ ê²€ìƒ‰ ìƒëµ
    if not is_recall_related_question(state["question"]):
        print(f"ì¼ë°˜ ì§ˆë¬¸ ê°ì§€ - ë¦¬ì½œ ê²€ìƒ‰ ìƒëµ")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }
    
    if recall_vectorstore is None:
        print("ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }
    
    try:
        # ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì¡°ê±´ ì²´í¬ (ì²« ì§ˆë¬¸ + ìµœì‹  ë°ì´í„° ìš”ì²­)
        chat_history = state.get("chat_history", [])
        is_first_question = len(chat_history) == 0
        
        recent_keywords = ["ìµœê·¼", "recent", "latest", "new", "ìƒˆë¡œìš´", "ìš”ì¦˜", "í˜„ì¬"]
        is_recent_query = any(keyword in state["question"].lower() for keyword in recent_keywords)
        
        # ì„¸ì…˜ ë‚´ì—ì„œ ì´ë¯¸ í¬ë¡¤ë§í–ˆëŠ”ì§€ í™•ì¸
        has_crawled_in_session = False
        for msg in chat_history:
            if isinstance(msg, AIMessage) and "âš¡ì‹¤ì‹œê°„:" in msg.content:
                has_crawled_in_session = True
                break
        
        should_crawl = is_recent_query and not has_crawled_in_session
        
        if should_crawl:
            print("ğŸ” ì²« ì§ˆë¬¸ + ìµœì‹  ë°ì´í„° ìš”ì²­ - ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìˆ˜í–‰")
            try:     
                crawler = get_crawler()
                # ë²¡í„°DBì˜ ìµœì‹  ë‚ ì§œ ì¡°íšŒ
                latest_date_in_db = get_latest_date_from_vectorstore(recall_vectorstore)
                new_recalls = crawler.crawl_latest_recalls(after_date=latest_date_in_db, vectorstore=recall_vectorstore)
                
                if new_recalls:
                    added_count = update_vectorstore_with_new_data(new_recalls, recall_vectorstore)
                    print(f"âœ… ìƒˆ ë°ì´í„° {added_count}ê±´ ì¶”ê°€ë¨")
                else:
                    print("ğŸ“‹ ìƒˆ ë¦¬ì½œ ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"âš ï¸ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        # ğŸ†• ì—¬ê¸°ì— ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€ (í¬ë¡¤ë§ ì§í›„)
        all_data = recall_vectorstore.get()
        metadatas = all_data.get('metadatas', [])
        
        # ìµœê·¼ ë°ì´í„° í™•ì¸
        latest_dates = []
        for metadata in metadatas:
            if metadata and metadata.get('effective_date'):
                date_str = metadata['effective_date']
                if date_str.startswith('2025'):  # 2025ë…„ ë°ì´í„°ë§Œ
                    latest_dates.append({
                        'date': date_str,
                        'title': metadata.get('title', '')[:50],
                        'source': metadata.get('source', ''),
                        'url': metadata.get('url', '')[-30:]
                    })
        
        # ë‚ ì§œìˆœ ì •ë ¬í•´ì„œ ìµœì‹  5ê°œ ì¶œë ¥
        latest_dates.sort(key=lambda x: x['date'], reverse=True)
        print("\nğŸ” ë²¡í„°DBì˜ 2025ë…„ ë°ì´í„° í™•ì¸:")
        for item in latest_dates[:5]:
            print(f"  ğŸ“… {item['date']} | {item['source']} | {item['title']} | {item['url']}")
        
        # ğŸ“… ìµœì‹  ë°ì´í„° ìš°ì„  ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ ìš°íšŒ)
        print("ğŸ“… ìµœì‹  ë°ì´í„° ìš°ì„  ê²€ìƒ‰...")

        # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_data = recall_vectorstore.get()
        all_documents = []

        for i, metadata in enumerate(all_data.get('metadatas', [])):
            if metadata:
                content = all_data.get('documents', [])[i] if i < len(all_data.get('documents', [])) else ""
                doc = Document(page_content=content, metadata=metadata)
                all_documents.append(doc)

        def get_date_for_sorting(doc):
            date_str = doc.metadata.get('effective_date', '1900-01-01')
            try:
                return datetime.strptime(date_str, '%Y-%m-%d')
            except:
                return datetime(1900, 1, 1)

        # ğŸ†• ì²­í¬ ì œê±° í›„ì—ëŠ” ë‹¨ìˆœí•œ ë‚ ì§œìˆœ ì •ë ¬ë§Œ í•„ìš”
        if len(all_documents) > 0 and 'chunk_index' in all_documents[0].metadata:
            # ê¸°ì¡´ ì²­í¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° - URLë³„ ì¤‘ë³µ ì œê±°
            url_groups = {}
            for doc in all_documents:
                url = doc.metadata.get('url', 'unknown')
                
                # URLë³„ë¡œ ê°€ì¥ ê¸´ contentë¥¼ ê°€ì§„ ì²­í¬ë§Œ ìœ ì§€
                if url not in url_groups or len(doc.page_content) > len(url_groups[url].page_content):
                    url_groups[url] = doc
            
            unique_recalls = list(url_groups.values())
            unique_recalls.sort(key=get_date_for_sorting, reverse=True)
            print(f"ğŸ“Š URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°: {len(unique_recalls)}ê°œ")
        else:
            # ì²­í¬ ì—†ëŠ” ìƒˆ ë°ì´í„° - ë°”ë¡œ ë‚ ì§œìˆœ ì •ë ¬
            unique_recalls = all_documents
            unique_recalls.sort(key=get_date_for_sorting, reverse=True)
            print(f"ğŸ“Š ë‹¨ì¼ ë¬¸ì„œ ì •ë ¬: {len(unique_recalls)}ê°œ")

        # ë””ë²„ê¹… ì¶œë ¥
        for i, doc in enumerate(unique_recalls[:10]):
            date = doc.metadata.get('effective_date', 'N/A')
            title = doc.metadata.get('title', '')[:50]
            source = doc.metadata.get('source', '')
            url_suffix = doc.metadata.get('url', '')[-30:]
            print(f"  {i+1}. {date} | {source} | {title}... | {url_suffix}")

        # ìƒìœ„ 5ê°œ ì„ íƒ
        selected_docs = unique_recalls[:5]

        print(f"\nğŸ¯ ìµœì¢… selected_docs:")
        for i, doc in enumerate(selected_docs):
            date = doc.metadata.get('effective_date', 'N/A')
            title = doc.metadata.get('title', '')[:50]
            source = doc.metadata.get('source', '')
            print(f"  {i+1}. {date} | {source} | {title}...")

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_parts = []
        for doc in selected_docs:
            content_with_meta = f"{doc.page_content}\nSource URL: {doc.metadata.get('url', 'N/A')}"
            context_parts.append(content_with_meta)

        context = "\n\n---\n\n".join(context_parts)
        
        print(f"ğŸ“Š ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(selected_docs)}ê±´")
        
        return {
            **state,
            "recall_context": context,
            "recall_documents": selected_docs
        }
        
    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }

#==============================================================
# ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
PROMPT_RECALL_ANSWER = """
ë‹¹ì‹ ì€ FDA ë¦¬ì½œÂ·íšŒìˆ˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë¦¬ì½œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“Œ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ "FDA Recall Database Information"ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
2. ë¦¬ì½œ ì‚¬ë¡€ê°€ 1ê±´ ì´ìƒì´ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤:
   | ë‚ ì§œ | ë¸Œëœë“œ | ì œí’ˆ | ë¦¬ì½œ ì‚¬ìœ  | ì¢…ë£Œ ì—¬ë¶€ | ì¶œì²˜ |
3. ì¶œì²˜ ë§í¬ê°€ ìˆìœ¼ë©´ í•˜ì´í¼ë§í¬ í˜•íƒœë¡œ í¬í•¨í•©ë‹ˆë‹¤.
4. ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ë§Œ ìˆìœ¼ë©´ "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ í•´ë‹¹ ì‚¬ë¡€ í™•ì¸ ë¶ˆê°€"ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤.
5. í‘œ ì•„ë˜ì— 3-5ë¬¸ì¥ìœ¼ë¡œ ì¢…í•© ìš”ì•½ì„ ì‘ì„±í•©ë‹ˆë‹¤.

ğŸ“ ì§ˆë¬¸: {question}

ğŸ“’ FDA Recall Database Information:
{recall_context}

ğŸ”½ ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
"""

PROMPT_GENERAL_QUESTION = """
ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:
"""

# ê¸°ì¡´ PROMPT_RECALL_ANSWER ë‹¤ìŒì— ì¶”ê°€
PROMPT_NEWS_ANSWER = """
ë‹¹ì‹ ì€ FDA ë¦¬ì½œÂ·íšŒìˆ˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ìµœì‹  ë‰´ìŠ¤ ì •ë³´ë¥¼ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë¦¬ì½œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“Œ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ "Latest News Information"ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
2. ë‰´ìŠ¤ ê¸°ì‚¬ê°€ 1ê±´ ì´ìƒì´ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤:
   | ë‚ ì§œ | ì¶œì²˜ | ì œí’ˆ/ë¸Œëœë“œ | ë¦¬ì½œ ì‚¬ìœ  | ë§í¬ |
3. ë‹µë³€ ì‹œì‘ ë¶€ë¶„ì— "ê´€ë ¨ ë¦¬ì½œ ì‚¬ë¡€ê°€ FDA ê³µì‹ ì‚¬ì´íŠ¸ì— ëª…ì‹œë˜ì–´ìˆì§€ ì•Šì•„ ê´€ë ¨ëœ ë‰´ìŠ¤ ì •ë³´ë¡œ ì œê³µí•©ë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤.
4. í•´ì™¸ ë¦¬ì½œ ì‚¬ë¡€ì˜ ê²½ìš° "í•´ì™¸ ì‚¬ë¡€ë¡œ êµ­ë‚´ ì§ì ‘ ì˜í–¥ ì—†ìŒ"ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤.
5. ê´€ë ¨ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ "í˜„ì¬ ë‰´ìŠ¤ ê¸°ì¤€ ê´€ë ¨ ì‚¬ë¡€ í™•ì¸ ë¶ˆê°€"ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤.
6. í‘œ ì•„ë˜ì— 3-5ë¬¸ì¥ìœ¼ë¡œ ì¢…í•© ìš”ì•½ ë° ì°¸ê³ ì‚¬í•­ì„ ì‘ì„±í•©ë‹ˆë‹¤.

ğŸ“ ì§ˆë¬¸: {question}

ğŸ“° Latest News Information:
{news_context}

ğŸ”½ ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
"""
#==============================================================

def google_news_search_node(state: RecallState) -> RecallState:
    """ì´ ì½”ë“œëŠ” êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ë¦¬ì½œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    
    try:
        # ğŸ†• ë‰´ìŠ¤ ê²€ìƒ‰ ì „ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
        clean_keywords = extract_question_keywords(state["question"])  # "ë§Œë‘ ë¦¬ì½œ ì‚¬ë¡€" â†’ "ë§Œë‘"
        
        print(f"ğŸ“° êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{clean_keywords}' (ì›ë³¸: '{state['question']}')")
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë³¸ë¬¸ ì¶”ì¶œ
        news_results = search_and_extract_news(clean_keywords, max_results=3)
        
        if news_results:
            # ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            news_context = format_news_for_context(news_results)
            print(f"âœ… êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(news_results)}ê±´")
            
            return {
                **state,
                "recall_context": "",  # ğŸ†• FDA ì»¨í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°
                "recall_documents": [],  # ğŸ†• FDA ë¬¸ì„œ ì™„ì „ ì œê±°
                "news_context": news_context,
                "news_documents": news_results
            }
        else:
            print("âŒ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {
                **state,
                "news_context": "",
                "news_documents": []
            }
            
    except Exception as e:
        print(f"êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "news_context": "",
            "news_documents": []
        }

def should_use_google_news(state: RecallState) -> str:
    """ì´ ì½”ë“œëŠ” êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤ - ìœ ì‚¬ë„ ê¸°ë°˜ íŒë‹¨"""
    
    # ë¦¬ì½œ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
    if not is_recall_related_question(state["question"]):
        print("ğŸ“ ì¼ë°˜ ì§ˆë¬¸ - ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì§í–‰")
        return "generate_answer"
    
    # ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    recall_docs = state.get("recall_documents", [])
    recall_count = len(recall_docs)
    
    print(f"ğŸ” ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼: {recall_count}ê±´")
    
    # ğŸ†• ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ë ¨ì„± ê²€ì‚¬
    if recall_count > 0:
        relevant_docs = check_document_relevance(state["question"], recall_docs)
        relevant_count = len(relevant_docs)
        
        print(f"ğŸ¯ ìœ ì‚¬ë„ ê²€ì‚¬ í›„ ê´€ë ¨ ë¬¸ì„œ: {relevant_count}ê±´")
        
        # ê´€ë ¨ ë¬¸ì„œê°€ 2ê±´ ë¯¸ë§Œì´ë©´ êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
        if relevant_count < 2:
            print("ğŸ“° ê´€ë ¨ ë¬¸ì„œ ë¶€ì¡± - êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰")
            return "google_search"
        else:
            print("ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ ì¶©ë¶„ - ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì§„í–‰")
            return "generate_answer"
    else:
        print("ğŸ“° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰")
        return "google_search"

def check_document_relevance(question: str, documents: List[Document]) -> List[Document]:
    """ì´ ì½”ë“œëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ LLMìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = extract_question_keywords(question)
        
        relevant_docs = []
        
        for i, doc in enumerate(documents):
            title = doc.metadata.get('title', '')
            content_preview = doc.page_content[:500]
            
            # ğŸ†• ê°œì„ ëœ ê´€ë ¨ì„± íŒë‹¨ í”„ë¡¬í”„íŠ¸
            relevance_prompt = f"""
                ë‹¤ìŒ ì§ˆë¬¸ê³¼ FDA ë¦¬ì½œ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì—„ê²©íˆ íŒë‹¨í•˜ì„¸ìš”.
                
                ì§ˆë¬¸: {question}
                í•µì‹¬ í‚¤ì›Œë“œ: {question_keywords}
                
                FDA ë¦¬ì½œ ë¬¸ì„œ:
                ì œëª©: {title}
                ë‚´ìš©: {content_preview}
                
                ì—„ê²©í•œ íŒë‹¨ ê¸°ì¤€:
                1. í•µì‹¬ í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ë‚´ìš©ì— ì§ì ‘ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
                2. ë™ì¼í•œ ì œí’ˆëª…/ë¸Œëœë“œëª…ì´ ì–¸ê¸‰ë˜ëŠ”ê°€?
                3. ê°™ì€ ì‹í’ˆ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œë„ êµ¬ì²´ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ê°€?
                
                ì˜ˆì‹œ:
                - ì§ˆë¬¸ "ë§Œë‘ ë¦¬ì½œ" vs ë¬¸ì„œ "dumpling recall" â†’ ê´€ë ¨
                - ì§ˆë¬¸ "ë§Œë‘ ë¦¬ì½œ" vs ë¬¸ì„œ "pasta recall" â†’ ë¬´ê´€
                - ì§ˆë¬¸ "ì‚¼ì–‘ ë¼ë©´" vs ë¬¸ì„œ "ë†ì‹¬ ë¼ë©´" â†’ ë¬´ê´€
                
                ë‹µë³€: "ê´€ë ¨" ë˜ëŠ” "ë¬´ê´€" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
                """
            
            response = llm.invoke([HumanMessage(content=relevance_prompt)])
            relevance = response.content.strip().lower()
            
            if "ê´€ë ¨" in relevance:
                relevant_docs.append(doc)
                print(f"    âœ… ê´€ë ¨ ë¬¸ì„œ {i+1}: {title[:50]}...")
            else:
                print(f"    âŒ ë¬´ê´€ ë¬¸ì„œ {i+1}: {title[:50]}...")
        
        return relevant_docs
        
    except Exception as e:
        print(f"ê´€ë ¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        return documents

def extract_question_keywords(question: str) -> str:
    """ì´ ì½”ë“œëŠ” ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ê°„ë‹¨ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    import re
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = ["ë¦¬ì½œ", "íšŒìˆ˜", "ì‚¬ë¡€", "ìˆë‚˜ìš”", "ì–´ë–¤", "ì–´ë–»ê²Œ", "ì–¸ì œ", "ì™œ", "ìµœê·¼", "ìš”ì¦˜", "í˜„ì¬"]
    
    # í•œê¸€ê³¼ ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
    words = re.findall(r'[ê°€-í£A-Za-z]+', question)
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    return " ".join(keywords[:3])

def answer_generation_node(state: RecallState) -> RecallState:
    """ì´ ì½”ë“œëŠ” ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    
    # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    is_recall_question = is_recall_related_question(state["question"])
    
    if not is_recall_question:
        # ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬
        try:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3)
            prompt = PromptTemplate.from_template(PROMPT_GENERAL_QUESTION)
            chain = prompt | llm | StrOutputParser()
            
            answer = chain.invoke({"question": state["question"]})
            final_answer = f"{answer}\n\nğŸ’¡ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ë¨"
            
            return {
                **state,
                "final_answer": final_answer
            }
            
        except Exception as e:
            return {
                **state,
                "final_answer": f"ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
            }
    
    # ë¦¬ì½œ ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        # ğŸ†• ì»¨í…ìŠ¤íŠ¸ ê²°ì • (FDA vs ë‰´ìŠ¤)
        recall_context = state.get("recall_context", "")
        news_context = state.get("news_context", "")

        print(f"ğŸ” recall_context ê¸¸ì´: {len(recall_context)}")
        print(f"ğŸ” news_context ê¸¸ì´: {len(news_context)}")
        
        if recall_context:
            print("ğŸ“‹ FDA ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ì„ íƒ")
            # FDA ë°ì´í„° ê¸°ë°˜ ë‹µë³€
            prompt = PromptTemplate.from_template(PROMPT_RECALL_ANSWER)
            context = recall_context
            source_type = "FDA ê³µì‹ ë°ì´í„°"
        elif news_context:
            print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ì„ íƒ")
            # ë‰´ìŠ¤ ë°ì´í„° ê¸°ë°˜ ë‹µë³€
            prompt = PromptTemplate.from_template(PROMPT_NEWS_ANSWER)  # ğŸ†• ë‰´ìŠ¤ìš© í”„ë¡¬í”„íŠ¸
            context = news_context
            source_type = "ìµœì‹  ë‰´ìŠ¤"
        else:
            return {
                **state,
                "final_answer": "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ë¦¬ì½œ ì‚¬ë¡€ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        chain = prompt | llm | StrOutputParser()
        
        answer = chain.invoke({
            "question": state["question"],
            "recall_context": context if recall_context else "",
            "news_context": context if news_context else ""
        })
        
        # ğŸ†• ê²€ìƒ‰ ì •ë³´ ì¶”ê°€
        search_info = f"\n\nğŸ“‹ ì •ë³´ ì¶œì²˜: {source_type}"
        
        if recall_context:
            recall_docs = state.get("recall_documents", [])
            if recall_docs:
                realtime_count = len([doc for doc in recall_docs 
                                   if doc.metadata.get("source") == "realtime_crawl"])
                search_info += f" (ì´ {len(recall_docs)}ê±´"
                if realtime_count > 0:
                    search_info += f", âš¡ì‹¤ì‹œê°„: {realtime_count}ê±´"
                search_info += ")"
        elif news_context:
            news_docs = state.get("news_documents", [])
            search_info += f" (ë‰´ìŠ¤ {len(news_docs)}ê±´)"
        
        final_answer = f"{answer}{search_info}"
        
        return {
            **state,
            "final_answer": final_answer
        }
        
    except Exception as e:
        return {
            **state,
            "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        }

def update_history_node(state: RecallState) -> RecallState:
    """ì´ ì½”ë“œëŠ” ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤"""
    try:
        current_history = state.get("chat_history", [])
        
        updated_history = current_history.copy()
        updated_history.append(HumanMessage(content=state["question"]))
        updated_history.append(AIMessage(content=state["final_answer"]))
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 8ê°œ ë©”ì‹œì§€)
        if len(updated_history) > 8:
            updated_history = updated_history[-8:]
        
        return {
            **state,
            "chat_history": updated_history
        }
        
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return state

# LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ë¶€ë¶„ ìˆ˜ì •
recall_workflow = StateGraph(RecallState)

# ë…¸ë“œ ì¶”ê°€
recall_workflow.add_node("translate", translation_node)
recall_workflow.add_node("recall_search", recall_search_node)
recall_workflow.add_node("google_search", google_news_search_node)  # ğŸ†• ì¶”ê°€
recall_workflow.add_node("generate_answer", answer_generation_node)
recall_workflow.add_node("update_history", update_history_node)

# ì—£ì§€ ìˆ˜ì •
recall_workflow.add_edge(START, "translate")
recall_workflow.add_edge("translate", "recall_search")

# ğŸ†• ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
recall_workflow.add_conditional_edges("recall_search", should_use_google_news, {
    "google_search": "google_search",
    "generate_answer": "generate_answer"
})

recall_workflow.add_edge("google_search", "generate_answer")  # ğŸ†• ì¶”ê°€
recall_workflow.add_edge("generate_answer", "update_history")
recall_workflow.add_edge("update_history", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
recall_graph = recall_workflow.compile()

def ask_recall_question(question: str, chat_history: List = None) -> Dict[str, Any]:
    """ì´ ì½”ë“œëŠ” ë¦¬ì½œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤"""
    if chat_history is None:
        chat_history = []
    
    try:
        result = recall_graph.invoke({
            "question": question,
            "question_en": "",  # ë²ˆì—­ ë…¸ë“œì—ì„œ ì±„ì›Œì§
            "recall_context": "",
            "recall_documents": [],
            "final_answer": "",
            "chat_history": chat_history
        })
        
        return {
            "answer": result["final_answer"],
            "recall_documents": result["recall_documents"],
            "chat_history": result["chat_history"]
        }
        
    except Exception as e:
        return {
            "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "recall_documents": [],
            "chat_history": chat_history
        }
