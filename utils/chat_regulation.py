# utils/chat_regulation.py

import json
import os
from functools import wraps
from dotenv import load_dotenv
from typing import TypedDict, List, Dict, Any 
from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE, DEFAULT_COLLECTION
from langchain_openai import OpenAIEmbeddings, ChatOpenAI 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate 
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from langchain_teddynote import logging   # LangSmith ì¶”ì  í™œì„±í™”

load_dotenv()                   # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
logging.langsmith("LLMPROJECT") # LangSmith ì¶”ì  ì„¤ì •

# ê³„ì¸µì  êµ¬ì¡°ë¥¼ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
CATEGORY_HIERARCHY = {
    "guidance": {
        "allergen": ["ì•ŒëŸ¬ì§€", "allergen", "ì•Œë ˆë¥´ê¸°", "ì•ŒëŸ¬ê²", "ê³¼ë¯¼ë°˜ì‘"],
        "additives": ["ì²¨ê°€ë¬¼", "additive", "ì‹í’ˆì²¨ê°€ë¬¼", "ë°©ë¶€ì œ", "ê°ë¯¸ë£Œ", "í–¥ë£Œ", "ì°©ìƒ‰ë£Œ"],
        "labeling": ["ë¼ë²¨ë§", "labeling", "ë¼ë²¨", "í‘œì‹œ", "ì˜ì–‘ì„±ë¶„", "ì›ì¬ë£Œ", "ì„±ë¶„í‘œì‹œ"],
        "main": ["ê°€ì´ë“œë¼ì¸", "guidance", "cpg", "ê°€ì´ë“œ", "ì¼ë°˜", "ì‹í’ˆê´€ë ¨", "food"]
    },
    "regulation": {
        "ecfr": ["ecfr", "ì—°ë°©ê·œì •ì§‘", "ì „ìì—°ë°©ê·œì •", "cfr"],
        "usc": ["21usc", "ë²•ë¥ ", "ì¡°í•­", "ê·œì •", "regulation", "ë²•ë ¹"]
    }
}

# í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ í•¨ìˆ˜
def translate_korean_to_english(korean_text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        prompt = f"Translate the following Korean text to English. Only return the translation without any explanation:\n\n{korean_text}"
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except Exception as e:
        print(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return korean_text

def initialize_chromadb_collection():
    """DuckDB ê¸°ë°˜ìœ¼ë¡œ ChromaDB ì—°ê²°"""
    try:
        persist_dir = "./data/chroma_db"

        # DuckDBë¥¼ ì“°ë ¤ë©´ Settingsì— chroma_db_impl='duckdb' ëª…ì‹œ
        client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_dir,
            anonymized_telemetry=False
        ))

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = Chroma(
            client=client,
            collection_name="chroma_regulations",
            embedding_function=embeddings,
            persist_directory=persist_dir
        )

        collection = vectorstore._collection
        document_count = collection.count()

        if document_count > 0:
            print(f"âœ… DuckDB ê¸°ë°˜ ChromaDB ì—°ê²° ì™„ë£Œ: {document_count}ê°œ ë¬¸ì„œ")
            return vectorstore
        else:
            raise ValueError("ChromaDB ì»¬ë ‰ì…˜ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ DuckDB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: str
    question_en: str
    document_type: str
    categories: List[str]
    chat_history: List[HumanMessage | AIMessage]
    context: str
    urls: List[str]
    answer: str
    need_synthesis: bool
    guidance_references: List[str]  # guidanceì—ì„œ regulation ì°¸ì¡°ë¥¼ ìœ„í•œ í•„ë“œ

# ë…¸ë“œ ì •ì˜
def router_node(state: GraphState) -> GraphState:
    """ì´ˆê¸° ë¼ìš°íŒ…: guidance vs regulation ê²°ì • + ë²ˆì—­"""
    question = state["question"].lower()
    
    # í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­
    try:
        question_en = translate_korean_to_english(state["question"])
        print(f"ë²ˆì—­ëœ ì§ˆë¬¸: {question_en}")
    except Exception as e:
        print(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        question_en = state["question"]
    
    # regulation í‚¤ì›Œë“œ ì²´í¬
    regulation_keywords = ["ë²•ë¥ ","ê·œì œ", "21usc", "ê·œì •", "regulation", "ë²•ë ¹", "ì¡°í•­", "cfr", "code of federal"]
    guidance_keywords = ["ê°€ì´ë“œ", "guidance", "cpg", "ì§€ì¹¨", "guideline"]
    
    combined_text = question + " " + question_en.lower()
    
    regulation_score = sum(1 for keyword in regulation_keywords if keyword in combined_text)
    guidance_score = sum(1 for keyword in guidance_keywords if keyword in combined_text)
    
    # ê¸°ë³¸ì ìœ¼ë¡œ guidance ìš°ì„ 
    document_type = "regulation" if regulation_score > guidance_score else "guidance"
    
    return {
        **state,
        "question_en": question_en,
        "document_type": document_type,
        "guidance_references": []
    }

def category_node(state: GraphState) -> GraphState:
    """ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ ë¶„ë¥˜ - ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬"""
    question = state["question"].lower()
    question_en = state["question_en"].lower()
    doc_type = state["document_type"]
    
    # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
    category_scores = {}
    category_keywords = CATEGORY_HIERARCHY[doc_type]
    
    # ì˜ì–´ í‚¤ì›Œë“œ ë§¤í•‘ í™•ì¥
    english_keywords = {
        "allergen": ["allergen", "allergy", "allergenic", "hypersensitivity", "allergic reaction"],
        "additives": ["additive", "preservatives", "sweetener", "flavoring", "coloring", "food additive"],
        "labeling": ["labeling", "label", "nutrition", "ingredient", "declaration", "nutritional facts"],
        "main": ["guidance", "general", "main", "comprehensive", "cpg", "food related"],
        "ecfr": ["electronic code", "federal regulations", "cfr", "code of federal regulations"],
        "usc": ["united states code", "federal law", "statute", "21 usc", "federal statute"]
    }
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
    for category, korean_keywords in category_keywords.items():
        score = 0
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in korean_keywords:
            if keyword.lower() in question:
                score += 2
        
        # ì˜ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in english_keywords.get(category, []):
            if keyword in question_en:
                score += 1.5
        
        category_scores[category] = score
    
    # ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬
    selected_categories = []
    
    # íŠ¹ë³„ íŒ¨í„´ ê°ì§€
    import re
    combined_text = question + " " + question_en.lower()
    
    complex_patterns = [
        (r'ì•ŒëŸ¬ì§€.*ê·œì œ|allergen.*regulation', 'allergen', 'guidance'),
        (r'ì²¨ê°€ë¬¼.*ê·œì œ|additive.*regulation', 'additives', 'guidance'), 
        (r'ë¼ë²¨ë§.*ê·œì œ|labeling.*regulation', 'labeling', 'guidance'),
    ]
    
    pattern_matched = False
    for pattern, target_category, target_doc_type in complex_patterns:
        if re.search(pattern, combined_text, re.IGNORECASE):
            selected_categories = [target_category]
            state["document_type"] = target_doc_type
            pattern_matched = True
            print(f"ë³µí•© ì§ˆë¬¸ ê°ì§€: '{target_category}' ì¹´í…Œê³ ë¦¬, '{target_doc_type}' ë¬¸ì„œíƒ€ì…ìœ¼ë¡œ ë³€ê²½")
            break
    
    if not pattern_matched:
        # ì¼ë°˜ ë¡œì§: ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì¹´í…Œê³ ë¦¬ë“¤ ì„ íƒ
        if category_scores:
            max_score = max(category_scores.values())
            if max_score > 0:
                threshold = max_score * 0.7
                selected_categories = [cat for cat, score in category_scores.items() 
                                     if score >= threshold]
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if not selected_categories:
        selected_categories = ["main"] if state["document_type"] == "guidance" else ["usc", "ecfr"]
    
    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ì„ íƒë˜ë©´ ì¢…í•©ì´ í•„ìš”
    need_synthesis = len(selected_categories) > 1
    
    print(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {selected_categories}, ë¬¸ì„œíƒ€ì…: {state['document_type']}, ì ìˆ˜: {category_scores}")
    
    return {
        **state,
        "categories": selected_categories,
        "need_synthesis": need_synthesis
    }

def document_retrieval_node(state: GraphState) -> GraphState:
    """ChromaDBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ - guidance â†’ regulation ì°¸ì¡° ë¡œì§ í¬í•¨"""
    all_documents = []
    guidance_references = []
    search_query = state["question_en"]
    
    for category in state["categories"]:
        docs_found = False
        
        # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (ChromaDBì— categoryí‚¤ì˜ ê°’ì´ ì†Œë¬¸ìë¡œ ì €ì¥ë˜ì–´ ìˆìŒ)
        try:
            filter_dict = {
                "$and": [
                    {"document_type": {"$eq": state["document_type"]}},
                    {"category": {"$eq": category.lower()}}  # ì†Œë¬¸ìë¡œ í†µì¼
                ]
            }
            
            retriever = vectorstore.as_retriever(
                search_kwargs={"k": 3, "filter": filter_dict}
            )
            
            # ì˜ì–´ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
            docs = retriever.invoke(search_query)
            
            if docs:
                all_documents.extend(docs)
                
                # guidance ë¬¸ì„œì—ì„œ regulation ì°¸ì¡° ì •ë³´ ì¶”ì¶œ
                if state["document_type"] == "guidance":
                    for doc in docs:
                        metadata = doc.metadata
                        # CFR ì°¸ì¡° ì¶”ì¶œ
                        cfr_refs = metadata.get("cfr_references", "")
                        if cfr_refs and cfr_refs.strip():
                            guidance_references.extend(cfr_refs.split(","))
                        
                        # USC ì°¸ì¡° ì¶”ì¶œ  
                        usc_refs = metadata.get("usc_references", "")
                        if usc_refs and usc_refs.strip():
                            guidance_references.extend(usc_refs.split(","))
                
                print(f"ì¹´í…Œê³ ë¦¬ '{category.lower()}'ì—ì„œ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
                docs_found = True
                
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ '{category}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    # 3ë‹¨ê³„: ë¬¸ì„œíƒ€ì…ë§Œìœ¼ë¡œ ê²€ìƒ‰
    if not all_documents:
        print(f"ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨. ë¬¸ì„œíƒ€ì… '{state['document_type']}'ìœ¼ë¡œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        try:
            type_filter = {"document_type": {"$eq": state["document_type"]}}
            type_retriever = vectorstore.as_retriever(
                search_kwargs={"k": 5, "filter": type_filter}
            )
            all_documents = type_retriever.invoke(search_query)
            print(f"ë¬¸ì„œíƒ€ì… ê²€ìƒ‰ì—ì„œ {len(all_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
        except Exception as e:
            print(f"ë¬¸ì„œíƒ€ì… ê²€ìƒ‰ë„ ì‹¤íŒ¨: {e}")
    
    # 4ë‹¨ê³„: ì „ì²´ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    if not all_documents:
        print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            general_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            all_documents = general_retriever.invoke(search_query)
            print(f"ì „ì²´ ê²€ìƒ‰ì—ì„œ {len(all_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
        except Exception as e:
            print(f"ì „ì²´ ê²€ìƒ‰ë„ ì‹¤íŒ¨: {e}")
    
    # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì„ íƒ
    unique_docs = []
    seen_content = set()
    for doc in all_documents:
        content_key = doc.page_content[:100]
        if content_key not in seen_content:
            unique_docs.append(doc)
            seen_content.add(content_key)
    
    selected_docs = unique_docs[:5]
    context = "\n\n".join([doc.page_content for doc in selected_docs])
    urls = list(set([doc.metadata.get("url", "") for doc in selected_docs if doc.metadata.get("url")]))
    
    # guidance_references ì •ë¦¬ (ì¤‘ë³µ ì œê±° ë° ê³µë°± ì œê±°)
    clean_references = []
    for ref in guidance_references:
        ref = ref.strip()
        if ref and ref not in clean_references:
            clean_references.append(ref)
    
    print(f"ìµœì¢…ì ìœ¼ë¡œ {len(selected_docs)}ê°œ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©")
    if clean_references:
        print(f"ì¶”ì¶œëœ regulation ì°¸ì¡°: {clean_references}")
    
    return {
        **state,
        "context": context,
        "urls": urls,
        "guidance_references": clean_references
    }

def synthesis_node(state: GraphState) -> GraphState:
    """guidance â†’ regulation ë‹¨ë°©í–¥ ì°¸ì¡°ë¥¼ í†µí•œ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ"""
    additional_context = ""
    additional_urls = []
    
    # guidance ë¬¸ì„œì—ì„œ regulation ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰
    if state["document_type"] == "guidance" and state["guidance_references"]:
        try:
            print(f"regulation ì°¸ì¡° ê²€ìƒ‰ ì‹œì‘: {state['guidance_references']}")
            
            # ì°¸ì¡°ëœ regulation ì„¹ì…˜ë“¤ì„ ê²€ìƒ‰
            for reference in state["guidance_references"]:
                reference = reference.strip()
                if not reference:
                    continue
                
                # CFR ì°¸ì¡°ì¸ì§€ USC ì°¸ì¡°ì¸ì§€ íŒë‹¨
                ref_lower = reference.lower()
                if "cfr" in ref_lower or "21 cfr" in ref_lower:
                    target_category = "ecfr"
                elif "usc" in ref_lower or "21 u.s.c" in ref_lower:
                    target_category = "usc"
                else:
                    # ê¸°ë³¸ì ìœ¼ë¡œ ë‘˜ ë‹¤ ê²€ìƒ‰
                    target_category = None
                
                # regulation ë¬¸ì„œì—ì„œ í•´ë‹¹ ì°¸ì¡° ê²€ìƒ‰
                try:
                    if target_category:
                        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰
                        reg_filter = {
                            "$and": [
                                {"document_type": {"$eq": "regulation"}},
                                {"category": {"$eq": target_category}}
                            ]
                        }
                    else:
                        # regulation ë¬¸ì„œ ì „ì²´ì—ì„œ ê²€ìƒ‰
                        reg_filter = {"document_type": {"$eq": "regulation"}}
                    
                    reg_retriever = vectorstore.as_retriever(
                        search_kwargs={"k": 2, "filter": reg_filter}
                    )
                    
                    # ì°¸ì¡° ë²ˆí˜¸ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
                    reg_docs = reg_retriever.invoke(reference)
                    
                    if reg_docs:
                        ref_context = f"\n\n[{reference} ê´€ë ¨ ê·œì •]\n"
                        ref_context += "\n".join([doc.page_content[:500] + "..." for doc in reg_docs])
                        additional_context += ref_context
                        
                        ref_urls = [doc.metadata.get("url", "") for doc in reg_docs if doc.metadata.get("url")]
                        additional_urls.extend(ref_urls)
                        
                        print(f"ì°¸ì¡° '{reference}'ì—ì„œ {len(reg_docs)}ê°œ regulation ë¬¸ì„œ ë°œê²¬")
                    
                except Exception as e:
                    print(f"ì°¸ì¡° '{reference}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¼ë°˜ì ì¸ ê´€ë ¨ regulation ê²€ìƒ‰ (ì°¸ì¡°ê°€ êµ¬ì²´ì ì´ì§€ ì•Šì€ ê²½ìš°)
            if not additional_context:
                try:
                    search_query = state["question_en"]
                    reg_filter = {"document_type": {"$eq": "regulation"}}
                    reg_retriever = vectorstore.as_retriever(
                        search_kwargs={"k": 2, "filter": reg_filter}
                    )
                    reg_docs = reg_retriever.invoke(search_query)
                    
                    if reg_docs:
                        additional_context = "\n\n[ê´€ë ¨ ê·œì • ì°¸ì¡°]\n"
                        additional_context += "\n".join([doc.page_content[:500] + "..." for doc in reg_docs])
                        additional_urls = [doc.metadata.get("url", "") for doc in reg_docs if doc.metadata.get("url")]
                        print(f"ì¼ë°˜ regulation ê²€ìƒ‰ì—ì„œ {len(reg_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                except Exception as e:
                    print(f"ì¼ë°˜ regulation ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        except Exception as e:
            print(f"guidance â†’ regulation ì°¸ì¡° ê²€ìƒ‰ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
    
    # ì¢…í•©ì´ í•„ìš”í•œ ê²½ìš° (ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬)
    elif state["need_synthesis"]:
        try:
            search_query = state["question_en"]
            cross_filter = {"document_type": {"$eq": state["document_type"]}}
            cross_retriever = vectorstore.as_retriever(
                search_kwargs={"k": 2, "filter": cross_filter}
            )
            cross_docs = cross_retriever.invoke(search_query)
            
            if cross_docs:
                additional_context = "\n\n[ì¶”ê°€ ê´€ë ¨ ì •ë³´]\n"
                additional_context += "\n".join([doc.page_content[:500] + "..." for doc in cross_docs])
                additional_urls = [doc.metadata.get("url", "") for doc in cross_docs if doc.metadata.get("url")]
        
        except Exception as e:
            print(f"ì¢…í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ì™€ URL ë³‘í•©
    if additional_context:
        updated_context = state["context"] + additional_context
        updated_urls = state["urls"] + additional_urls
        
        return {
            **state,
            "context": updated_context,
            "urls": updated_urls
        }
    
    return state

def generate_answer(state: GraphState) -> GraphState:
    """ë‹µë³€ ìƒì„±"""
    doc_info = f"ë¬¸ì„œ íƒ€ì…: {state['document_type']}, ì¹´í…Œê³ ë¦¬: {', '.join(state['categories'])}"
    
    # guidance â†’ regulation ì°¸ì¡° ì •ë³´ ì¶”ê°€
    if state["guidance_references"]:
        doc_info += f", ì°¸ì¡°ëœ regulation: {', '.join(state['guidance_references'])}"
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
    chat_history_text = ""
    if state.get("chat_history"):
        recent_history = state["chat_history"][-4:]
        chat_history_text = "\n".join([f"{msg.__class__.__name__}: {msg.content}" for msg in recent_history])
    
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë¯¸êµ­ FDA ê·œì œë¥¼ ì „ë¬¸ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê·œì œ ìë¬¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ **í•œêµ­ì–´ë¡œ ì •ë°€í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” í•´ì„**ì„ ì œê³µí•˜ì„¸ìš”.
â—ï¸ê·œì¹™:
- ë°˜ë“œì‹œ ê·œì œ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ì¶œì²˜ê°€ í¬í•¨ëœ ì¡°í•­ì€ **ì¸ìš© í‘œì‹œ(ì˜ˆ: 21 U.S.C. Â§ 721(b)(1))**ë¡œ ëª…ì‹œí•˜ê³ , ê°€ëŠ¥í•  ê²½ìš° í•´ë‹¹ ì¡°í•­ì˜ **URL ë§í¬ë„ í•¨ê»˜ ì œì‹œ**í•˜ì„¸ìš”.
- ì¶œì²˜ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° **ê´„í˜¸ ì—†ì´ ë§ˆë¬´ë¦¬**í•˜ì„¸ìš”.
- ì¤‘ìš” ë‚´ìš©ì€ **í•­ëª© ë˜ëŠ” ë²ˆí˜¸ í˜•ì‹**ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , êµ¬ì²´ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, "**ê´€ë ¨ ë¬¸ì„œì—ì„œ ëª…í™•í•œ ê¸°ì¤€ì€ í™•ì¸ë˜ì§€ ì•ŠìŒ**"ì´ë¼ê³  ì„œìˆ í•˜ì„¸ìš”.
- ë§ˆì§€ë§‰ì—ëŠ” ìœ„ì˜ í•­ëª©ë“¤ì„ **ìš”ì•½í•˜ì—¬ ì •ë¦¬í•œ ì¢…í•©ì  ë¶„ì„ ë¬¸ë‹¨**ì„ ì¶”ê°€í•˜ì„¸ìš”. (3~5ë¬¸ì¥ ì •ë„, í•µì‹¬ ë…¼ì ì„ ì„œìˆ ì ìœ¼ë¡œ ì„¤ëª…)

ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ğŸ“š ê´€ë ¨ ë¬¸ì„œ ì •ë³´:
{doc_info}

ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:
{context}

ğŸ’¬ ì´ì „ ëŒ€í™” ê¸°ë¡ (ìˆì„ ê²½ìš°):
{chat_history}

ğŸ”½ ì´ì œ ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë¦¬ëœ ì „ë¬¸ì  ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""
    )
    
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        chain = prompt | llm | StrOutputParser()
        
        answer = chain.invoke({
            "question": state["question"],
            "context": state["context"],
            "chat_history": chat_history_text,
            "doc_info": doc_info
        })
        
        # URL ì •ë³´ ì¶”ê°€
        if state["urls"]:
            unique_urls = list(set([url for url in state["urls"] if url.strip()]))
            if unique_urls:
                url_text = "\n\nğŸ“ ì¶œì²˜:\n" + "\n".join([f"- {url}" for url in unique_urls])
                full_answer = f"{answer}{url_text}"
            else:
                full_answer = answer
        else:
            full_answer = answer
        
        return {
            **state,
            "answer": full_answer
        }
    
    except Exception as e:
        error_answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        return {
            **state,
            "answer": error_answer
        }

def update_chat_history(state: GraphState) -> GraphState:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
    try:
        current_history = state.get("chat_history", [])
        
        # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
        updated_history = current_history.copy()
        updated_history.append(HumanMessage(content=state["question"]))
        updated_history.append(AIMessage(content=state["answer"]))
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 10ê°œ ë©”ì‹œì§€)
        if len(updated_history) > 10:
            updated_history = updated_history[-10:]
        
        return {
            **state,
            "chat_history": updated_history
        }
    
    except Exception as e:
        print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return state

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("router", router_node)
workflow.add_node("category", category_node) 
workflow.add_node("retrieval", document_retrieval_node)
workflow.add_node("synthesis", synthesis_node)
workflow.add_node("generate", generate_answer)
workflow.add_node("update_history", update_chat_history)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "router")
workflow.add_edge("router", "category")
workflow.add_edge("category", "retrieval")
workflow.add_edge("retrieval", "synthesis")
workflow.add_edge("synthesis", "generate")
workflow.add_edge("generate", "update_history")
workflow.add_edge("update_history", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def ask_question(question: str, chat_history: List = None) -> Dict[str, Any]:
    """ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    if chat_history is None:
        chat_history = []
    
    try:
        result = graph.invoke({
            "question": question,
            "question_en": "",
            "chat_history": chat_history,
            "document_type": "",
            "categories": [],
            "context": "",
            "urls": [],
            "answer": "",
            "need_synthesis": False,
            "guidance_references": []
        })
        
        return {
            "answer": result["answer"],
            "document_type": result["document_type"],
            "categories": result["categories"],
            "urls": result["urls"],
            "chat_history": result["chat_history"],
            "guidance_references": result["guidance_references"]
        }
    
    except Exception as e:
        return {
            "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "document_type": "",
            "categories": [],
            "urls": [],
            "chat_history": chat_history,
            "guidance_references": []
        }
