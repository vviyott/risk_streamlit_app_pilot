# components/news.py

import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def fetch_articles_with_keyword(keyword=None, max_pages=5, max_articles=3):
    base_url = "https://www.thinkfood.co.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm"
    results = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}&page={page}"
        res = requests.get(url)
        if res.status_code != 200:
            continue

        soup = BeautifulSoup(res.text, "html.parser")
        articles = soup.select(".list-block")

        for article in articles:
            if len(results) >= max_articles:
                return results  # ê¸°ì‚¬ 3ê°œ ëª¨ì´ë©´ ë°”ë¡œ ë°˜í™˜

            title_tag = article.select_one(".list-titles strong")
            link_tag = article.select_one(".list-titles a")
            summary_tag = article.select_one(".line-height-3-2x")
            date_tag = article.select_one(".list-dated")

            if not title_tag or not link_tag:
                continue

            title = title_tag.get_text(strip=True)
            # keywordê°€ Noneì´ ì•„ë‹ ê²½ìš°ì—ë§Œ í•„í„°ë§ ì ìš©
            if keyword is not None and keyword not in title:
                continue

            link = "https://www.thinkfood.co.kr" + link_tag["href"]
            summary = (summary_tag.get_text(strip=True)[:200] + "...") if summary_tag else ""
            info = date_tag.get_text(strip=True) if date_tag else ""

            # ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            img_url = None
            try:
                res_detail = requests.get(link)
                soup_detail = BeautifulSoup(res_detail.text, "html.parser")
                img_tag = soup_detail.select_one("figure img")
                if img_tag and "src" in img_tag.attrs:
                    src = img_tag["src"]
                    img_url = src if src.startswith("http") else "https://cdn.thinkfood.co.kr" + src
            except:
                pass # ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ì´ë¯¸ì§€ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬

            results.append({
                "title": title,
                "summary": summary,
                "info": info,
                "link": link,
                "img_url": img_url
            })

    return results

def fetch_full_article_content(url):
    """ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        res = requests.get(url)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        
        # ë°©ë²• 1: div.user-snip ì‹œë„
        content_div = soup.select_one("div.user-snip")
        if content_div:
            # ê´‘ê³ ë‚˜ ê´€ë ¨ ê¸°ì‚¬ ë§í¬ ì œê±°
            for unwanted in content_div.select('.ad, .related, .link-area, .photo-info'):
                unwanted.decompose()
            
            # ë³¸ë¬¸ p íƒœê·¸ë“¤ë§Œ ì¶”ì¶œ
            paragraphs = content_div.select('p')
            if paragraphs:
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            else:
                # p íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = content_div.get_text(strip=True)
        
        # ë°©ë²• 2: article íƒœê·¸ ì‹œë„
        if not content:
            article_tag = soup.select_one('article')
            if article_tag:
                paragraphs = article_tag.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
        
        # ë°©ë²• 3: .article-content ê°™ì€ í´ë˜ìŠ¤ ì‹œë„
        if not content:
            for selector in ['.article-content', '.news-content', '.content']:
                content_area = soup.select_one(selector)
                if content_area:
                    paragraphs = content_area.select('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                    if content:
                        break
        
        if content:
            # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            content = re.sub(r'\s+', ' ', content).strip()
            # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ì œì™¸
            if len(content) > 100:
                return content
        return None
    except Exception as e:
        print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def summarize_with_openai(content, openai_api_key):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸êµ­ ì‹í’ˆ ì‹œì¥ ê¸°ì‚¬ ìš”ì•½"""
    try:
        from openai import OpenAI

        client = OpenAI(api_key=openai_api_key)

        prompt = f"""
        ë‹¤ìŒì€ ìµœê·¼ ë¯¸êµ­ ì‹í’ˆ ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ì…ë‹ˆë‹¤.

        ë‹¤ìŒ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œ ìš”ì•½ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”:

        1. í˜„ì¬ëŠ” 2025ë…„ì…ë‹ˆë‹¤. **2024ë…„ ì´í›„ì˜ ë‚´ìš©ë§Œ í¬í•¨**í•˜ê³ , 2023ë…„ ì´ì „ì˜ ìˆ˜ì¹˜, ì‚¬ë¡€, íŠ¸ë Œë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        2. **ê¸°ì‚¬ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©**í•˜ê³ , ì„ì˜ì˜ ì¶”ë¡ ì´ë‚˜ í—ˆêµ¬ì˜ ìˆ˜ì¹˜ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        3. ë¬¸ì²´ëŠ” ê°ê´€ì ì´ê³  ë¶€ë“œëŸ½ê²Œ, ì¤‘ë¦½ì  í†¤ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”. (~í•  ìˆ˜ ìˆë‹¤, ~ë¡œ ë³´ì¸ë‹¤ ë“±)
        4. ê°ê°ì˜ í•­ëª©ì€ ë‹¨ë¬¸ ë‚˜ì—´ì´ ì•„ë‹ˆë¼ **ë¶€ë“œëŸ¬ìš´ ì—°ê²°ì–´ì™€ ë¬¸ì¥ íë¦„**ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
        5. ë…ìì—ê²Œ â€˜ì„¤ëª…í•´ì£¼ëŠ” ëŠë‚Œâ€™ìœ¼ë¡œ, **ì „ë¬¸ì„±ì´ ìˆìœ¼ë©´ì„œë„ ê³¼í•˜ì§€ ì•Šì€ ë”°ëœ»í•œ ì–´ì¡°**ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

        ---

        ìš”ì•½ì€ ì•„ë˜ 4ê°œì˜ í•­ëª©ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ê° í•­ëª©ì€ 2~3ë¬¸ì¥ ë‚´ì™¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤:

        **ì‹œì¥ í™˜ê²½ ë³€í™”**  
        **ì£¼ìš” íŠ¸ë Œë“œì™€ ì‚¬ë¡€**  
        **ì‚°ì—… ê³¼ì œ ë° ë¦¬ìŠ¤í¬**  
        **ì‹œì‚¬ì  ë° ì „ë§**

        ì „ì²´ëŠ” 6~9ë¬¸ì¥ ë‚´ì™¸ë¡œ êµ¬ì„±í•˜ë©°, **êµµì€ ì œëª©ìœ¼ë¡œ ë¬¸ë‹¨ì„ êµ¬ë¶„**í•˜ê³ ,  
        ë³´ê³ ì„œì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ì‹œì¥ ë¶„ì„ì„ ì‘ì„±í•©ë‹ˆë‹¤.

        ê¸°ì‚¬ ë³¸ë¬¸:
        {content}
        """

        system_msg = (
            "ë‹¹ì‹ ì€ ë¯¸êµ­ ì‹í’ˆ ì‚°ì—… ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
            "ê¸°ì‚¬ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ ë³€í™”, íŠ¸ë Œë“œ, ë¦¬ìŠ¤í¬, ì‹œì‚¬ì ì„ ì—°ê²°í•´ í†µì°°ë ¥ ìˆê²Œ ìš”ì•½í•©ë‹ˆë‹¤. "
            "ì‚¬ë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ê³ , ì „ëµ ì œì•ˆì€ í˜„ì‹¤ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        )

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
            max_tokens=700,
            temperature=0.3
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def show_news():
    st.info("""
    ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í†µí•´ ì„¸ê³„ ì‹í’ˆ ì‹œì¥ì˜ íë¦„ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n
    AIê°€ ë¶„ì„í•œ ë¯¸êµ­ì˜ ì£¼ìš” ì´ìŠˆ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ë¥¼ í•¨ê»˜ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    openai_api_key = os.getenv('OPENAI_API_KEY')

    articles = fetch_articles_with_keyword(keyword=None, max_pages=5, max_articles=3)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**ğŸ“° ì„¸ê³„ ë‰´ìŠ¤ ê¸°ì‚¬**")
        for article in articles:
            summary = article["summary"]
            if article["img_url"]:
                cols = st.columns([1, 3])
                with cols[0]:
                    st.image(article["img_url"], use_container_width=True)
                with cols[1]:
                    st.markdown(f"**[{article['title']}]({article['link']})**", unsafe_allow_html=True)
                    st.markdown(summary)
                    st.caption(article["info"])
            else:
                st.markdown(f"**[{article['title']}]({article['link']})**", unsafe_allow_html=True)
                st.markdown(summary)
                st.caption(article["info"])
            st.markdown("---")
        st.caption("ì¶œì²˜: [ì‹í’ˆìŒë£Œì‹ ë¬¸](https://www.thinkfood.co.kr/)")

    with col2:
        st.markdown("**ğŸ“Š ë¯¸êµ­ ì‹í’ˆ ì‚°ì—… ë™í–¥ ìš”ì•½ ë¶„ì„ (by OpenAI)**")
        with st.spinner("ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹í’ˆ ì‚°ì—… ë™í–¥ ë¶„ì„ ì¤‘..."):

            # ìš”ì•½ìš© ê¸°ì‚¬ ìˆ˜ì§‘
            articles_for_summary = fetch_articles_with_keyword(keyword="ë¯¸êµ­", max_pages=10, max_articles=10)

            # ë³¸ë¬¸ ìˆ˜ì§‘
            contents = []
            for article in articles_for_summary:
                full_content = fetch_full_article_content(article["link"])
                if full_content:
                    contents.append(full_content)

            combined = "\n\n".join(contents)
            summary_result = summarize_with_openai(combined, openai_api_key) if contents else "ìš”ì•½í•  ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        st.success(summary_result)

        with st.expander("ğŸ” ìš”ì•½ì— ì‚¬ìš©ëœ ê¸°ì‚¬ ì¶œì²˜ ë³´ê¸°"):
            for article in articles_for_summary:
                st.markdown(f"- [{article['title']}]({article['link']})", unsafe_allow_html=True)
